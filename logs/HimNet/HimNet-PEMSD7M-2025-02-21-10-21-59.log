PEMSD7M
Trainset:	x-(7589, 12, 228, 3)	y-(7589, 12, 228, 3)
Valset:  	x-(2530, 12, 228, 3)  	y-(2530, 12, 228, 3)
Testset:	x-(2530, 12, 228, 3)	y-(2530, 12, 228, 3)

Random seed = 233
--------- HimNet ---------
{
    "num_nodes": 228,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "himnet",
    "lr": 0.001,
    "eps": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        30,
        50
    ],
    "clip_grad": 5,
    "batch_size": 16,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_nodes": 228,
        "input_dim": 3,
        "output_dim": 1,
        "tod_embedding_dim": 8,
        "dow_embedding_dim": 8,
        "out_steps": 12,
        "hidden_dim": 64,
        "num_layers": 1,
        "cheb_k": 2,
        "ycov_dim": 2,
        "node_embedding_dim": 16,
        "st_embedding_dim": 16,
        "tf_decay_steps": 6000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
HimNet                                   [16, 12, 228, 1]          3,648
├─Embedding: 1-1                         [16, 8]                   2,304
├─Embedding: 1-2                         [16, 8]                   56
├─HimEncoder: 1-3                        [16, 12, 228, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─HimGCRU: 3-1                 [16, 228, 64]             414,720
│    │    └─HimGCRU: 3-2                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-3                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-4                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-5                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-6                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-7                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-8                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-9                 [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-10                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-11                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-12                [16, 228, 64]             (recursive)
├─HimEncoder: 1-4                        [16, 12, 228, 64]         --
│    └─ModuleList: 2-2                   --                        --
│    │    └─HimGCRU: 3-13                [16, 228, 64]             414,720
│    │    └─HimGCRU: 3-14                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-15                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-16                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-17                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-18                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-19                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-20                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-21                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-22                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-23                [16, 228, 64]             (recursive)
│    │    └─HimGCRU: 3-24                [16, 228, 64]             (recursive)
├─Linear: 1-5                            [16, 228, 16]             1,040
├─HimDecoder: 1-6                        [16, 228, 64]             --
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-25                [16, 228, 64]             414,720
├─Linear: 1-7                            [16, 228, 1]              65
├─HimDecoder: 1-8                        [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-26                [16, 228, 64]             (recursive)
├─Linear: 1-9                            [16, 228, 1]              (recursive)
├─HimDecoder: 1-10                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-27                [16, 228, 64]             (recursive)
├─Linear: 1-11                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-12                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-28                [16, 228, 64]             (recursive)
├─Linear: 1-13                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-14                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-29                [16, 228, 64]             (recursive)
├─Linear: 1-15                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-16                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-30                [16, 228, 64]             (recursive)
├─Linear: 1-17                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-18                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-31                [16, 228, 64]             (recursive)
├─Linear: 1-19                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-20                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-32                [16, 228, 64]             (recursive)
├─Linear: 1-21                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-22                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-33                [16, 228, 64]             (recursive)
├─Linear: 1-23                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-24                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-34                [16, 228, 64]             (recursive)
├─Linear: 1-25                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-26                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-35                [16, 228, 64]             (recursive)
├─Linear: 1-27                           [16, 228, 1]              (recursive)
├─HimDecoder: 1-28                       [16, 228, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-36                [16, 228, 64]             (recursive)
├─Linear: 1-29                           [16, 228, 1]              (recursive)
==========================================================================================
Total params: 1,251,273
Trainable params: 1,251,273
Non-trainable params: 0
Total mult-adds (G): 54.46
==========================================================================================
Input size (MB): 0.88
Forward/backward pass size (MB): 202.54
Params size (MB): 4.99
Estimated Total Size (MB): 208.41
==========================================================================================

Loss: MaskedMAELoss

2025-02-21 10:23:28.785506 Epoch 1  	Train Loss = 1.80394 Val Loss = 3.68185
2025-02-21 10:24:44.741995 Epoch 2  	Train Loss = 1.32723 Val Loss = 3.64624
2025-02-21 10:25:58.148215 Epoch 3  	Train Loss = 1.30357 Val Loss = 3.85433
2025-02-21 10:27:10.227957 Epoch 4  	Train Loss = 1.28524 Val Loss = 3.20860
2025-02-21 10:28:22.913250 Epoch 5  	Train Loss = 1.27548 Val Loss = 4.19545
2025-02-21 10:29:39.202492 Epoch 6  	Train Loss = 1.26747 Val Loss = 3.49223
2025-02-21 10:30:53.201954 Epoch 7  	Train Loss = 1.25876 Val Loss = 3.48154
2025-02-21 10:32:10.652167 Epoch 8  	Train Loss = 1.25434 Val Loss = 3.34455
2025-02-21 10:33:24.873975 Epoch 9  	Train Loss = 1.24571 Val Loss = 3.26755
2025-02-21 10:34:38.451535 Epoch 10  	Train Loss = 1.24140 Val Loss = 3.13807
2025-02-21 10:35:53.208573 Epoch 11  	Train Loss = 1.23641 Val Loss = 2.99826
2025-02-21 10:37:10.087034 Epoch 12  	Train Loss = 1.23151 Val Loss = 3.06889
2025-02-21 10:38:25.266566 Epoch 13  	Train Loss = 1.22691 Val Loss = 3.03750
2025-02-21 10:39:41.927390 Epoch 14  	Train Loss = 1.22330 Val Loss = 2.93308
2025-02-21 10:40:55.699030 Epoch 15  	Train Loss = 1.21822 Val Loss = 3.04119
2025-02-21 10:42:07.977654 Epoch 16  	Train Loss = 1.21430 Val Loss = 3.02693
2025-02-21 10:43:22.165431 Epoch 17  	Train Loss = 1.21258 Val Loss = 2.99454
2025-02-21 10:44:37.420004 Epoch 18  	Train Loss = 1.20924 Val Loss = 2.82978
2025-02-21 10:45:49.957986 Epoch 19  	Train Loss = 1.20543 Val Loss = 2.90669
2025-02-21 10:47:02.411466 Epoch 20  	Train Loss = 1.20394 Val Loss = 2.80388
2025-02-21 10:48:14.113940 Epoch 21  	Train Loss = 1.19692 Val Loss = 2.98999
2025-02-21 10:49:28.680862 Epoch 22  	Train Loss = 1.19457 Val Loss = 3.78859
2025-02-21 10:50:42.612987 Epoch 23  	Train Loss = 1.19074 Val Loss = 2.84186
2025-02-21 10:51:58.115844 Epoch 24  	Train Loss = 1.18537 Val Loss = 3.00946
2025-02-21 10:53:12.217405 Epoch 25  	Train Loss = 1.18083 Val Loss = 2.87756
2025-02-21 10:54:26.081023 Epoch 26  	Train Loss = 1.17546 Val Loss = 2.75895
2025-02-21 10:55:43.857527 Epoch 27  	Train Loss = 1.17309 Val Loss = 2.98258
2025-02-21 10:56:58.379663 Epoch 28  	Train Loss = 1.16758 Val Loss = 2.89526
2025-02-21 10:58:12.221634 Epoch 29  	Train Loss = 1.16495 Val Loss = 2.85309
2025-02-21 10:59:26.612560 Epoch 30  	Train Loss = 1.15937 Val Loss = 2.76899
2025-02-21 11:00:39.688397 Epoch 31  	Train Loss = 1.14221 Val Loss = 2.66171
2025-02-21 11:01:53.446629 Epoch 32  	Train Loss = 1.14050 Val Loss = 2.66636
2025-02-21 11:03:06.216391 Epoch 33  	Train Loss = 1.13918 Val Loss = 2.67386
2025-02-21 11:04:18.583632 Epoch 34  	Train Loss = 1.13841 Val Loss = 2.67210
2025-02-21 11:05:31.130299 Epoch 35  	Train Loss = 1.13746 Val Loss = 2.67829
2025-02-21 11:06:43.983398 Epoch 36  	Train Loss = 1.13767 Val Loss = 2.66454
2025-02-21 11:07:56.581909 Epoch 37  	Train Loss = 1.13628 Val Loss = 2.65888
2025-02-21 11:09:09.355332 Epoch 38  	Train Loss = 1.13559 Val Loss = 2.65245
2025-02-21 11:10:22.838495 Epoch 39  	Train Loss = 1.13498 Val Loss = 2.67975
2025-02-21 11:11:36.993987 Epoch 40  	Train Loss = 1.13468 Val Loss = 2.67478
2025-02-21 11:12:49.409764 Epoch 41  	Train Loss = 1.13363 Val Loss = 2.66359
2025-02-21 11:14:02.498201 Epoch 42  	Train Loss = 1.13331 Val Loss = 2.63291
2025-02-21 11:15:16.768315 Epoch 43  	Train Loss = 1.13310 Val Loss = 2.64661
2025-02-21 11:16:31.683743 Epoch 44  	Train Loss = 1.13191 Val Loss = 2.64130
2025-02-21 11:17:46.085765 Epoch 45  	Train Loss = 1.13097 Val Loss = 2.65212
2025-02-21 11:18:58.409698 Epoch 46  	Train Loss = 1.13104 Val Loss = 2.64982
2025-02-21 11:20:11.214887 Epoch 47  	Train Loss = 1.13118 Val Loss = 2.63326
2025-02-21 11:21:23.404242 Epoch 48  	Train Loss = 1.13066 Val Loss = 2.66745
2025-02-21 11:22:38.912787 Epoch 49  	Train Loss = 1.12952 Val Loss = 2.65306
2025-02-21 11:23:54.663051 Epoch 50  	Train Loss = 1.12769 Val Loss = 2.64889
2025-02-21 11:25:09.076939 Epoch 51  	Train Loss = 1.12729 Val Loss = 2.63270
2025-02-21 11:26:22.922886 Epoch 52  	Train Loss = 1.12659 Val Loss = 2.63501
2025-02-21 11:27:36.879401 Epoch 53  	Train Loss = 1.12709 Val Loss = 2.63622
2025-02-21 11:28:52.945912 Epoch 54  	Train Loss = 1.12695 Val Loss = 2.63768
2025-02-21 11:30:06.969557 Epoch 55  	Train Loss = 1.12779 Val Loss = 2.63354
2025-02-21 11:31:21.320178 Epoch 56  	Train Loss = 1.12691 Val Loss = 2.63669
2025-02-21 11:32:36.779494 Epoch 57  	Train Loss = 1.12865 Val Loss = 2.63516
2025-02-21 11:33:53.048959 Epoch 58  	Train Loss = 1.13025 Val Loss = 2.63667
2025-02-21 11:35:11.184366 Epoch 59  	Train Loss = 1.12878 Val Loss = 2.63416
2025-02-21 11:36:28.221621 Epoch 60  	Train Loss = 1.12921 Val Loss = 2.62805
2025-02-21 11:37:41.754464 Epoch 61  	Train Loss = 1.13117 Val Loss = 2.64017
2025-02-21 11:38:53.497758 Epoch 62  	Train Loss = 1.13090 Val Loss = 2.63083
2025-02-21 11:40:06.858515 Epoch 63  	Train Loss = 1.13285 Val Loss = 2.63009
2025-02-21 11:41:20.415905 Epoch 64  	Train Loss = 1.13132 Val Loss = 2.63090
2025-02-21 11:42:34.253578 Epoch 65  	Train Loss = 1.13479 Val Loss = 2.63034
2025-02-21 11:43:47.130283 Epoch 66  	Train Loss = 1.13408 Val Loss = 2.62964
2025-02-21 11:45:02.698818 Epoch 67  	Train Loss = 1.13425 Val Loss = 2.62708
2025-02-21 11:46:17.126912 Epoch 68  	Train Loss = 1.13861 Val Loss = 2.62783
2025-02-21 11:47:30.169561 Epoch 69  	Train Loss = 1.13536 Val Loss = 2.62487
2025-02-21 11:48:43.928485 Epoch 70  	Train Loss = 1.13967 Val Loss = 2.63167
2025-02-21 11:49:56.591001 Epoch 71  	Train Loss = 1.14057 Val Loss = 2.62820
2025-02-21 11:51:10.221589 Epoch 72  	Train Loss = 1.14099 Val Loss = 2.62726
2025-02-21 11:52:24.542720 Epoch 73  	Train Loss = 1.14506 Val Loss = 2.62906
2025-02-21 11:53:39.610616 Epoch 74  	Train Loss = 1.14681 Val Loss = 2.62846
2025-02-21 11:54:52.614820 Epoch 75  	Train Loss = 1.14946 Val Loss = 2.62673
2025-02-21 11:56:07.527357 Epoch 76  	Train Loss = 1.15031 Val Loss = 2.62476
2025-02-21 11:57:20.668907 Epoch 77  	Train Loss = 1.15366 Val Loss = 2.61878
2025-02-21 11:58:33.070997 Epoch 78  	Train Loss = 1.15374 Val Loss = 2.62517
2025-02-21 11:59:46.215041 Epoch 79  	Train Loss = 1.15538 Val Loss = 2.63004
2025-02-21 12:01:01.812996 Epoch 80  	Train Loss = 1.16010 Val Loss = 2.61730
2025-02-21 12:02:16.957080 Epoch 81  	Train Loss = 1.16123 Val Loss = 2.61823
2025-02-21 12:03:32.428309 Epoch 82  	Train Loss = 1.16909 Val Loss = 2.61735
2025-02-21 12:04:45.786756 Epoch 83  	Train Loss = 1.16664 Val Loss = 2.61735
2025-02-21 12:06:01.479000 Epoch 84  	Train Loss = 1.17852 Val Loss = 2.62088
2025-02-21 12:07:18.699871 Epoch 85  	Train Loss = 1.17739 Val Loss = 2.60677
2025-02-21 12:08:33.062131 Epoch 86  	Train Loss = 1.18015 Val Loss = 2.61386
2025-02-21 12:09:47.155124 Epoch 87  	Train Loss = 1.18275 Val Loss = 2.60848
2025-02-21 12:11:01.961267 Epoch 88  	Train Loss = 1.19330 Val Loss = 2.60647
2025-02-21 12:12:19.018512 Epoch 89  	Train Loss = 1.19674 Val Loss = 2.61343
2025-02-21 12:13:32.266335 Epoch 90  	Train Loss = 1.20306 Val Loss = 2.60270
2025-02-21 12:14:47.066752 Epoch 91  	Train Loss = 1.21180 Val Loss = 2.61210
2025-02-21 12:16:00.274949 Epoch 92  	Train Loss = 1.21724 Val Loss = 2.59932
2025-02-21 12:17:15.654596 Epoch 93  	Train Loss = 1.21897 Val Loss = 2.61045
2025-02-21 12:18:29.443668 Epoch 94  	Train Loss = 1.23229 Val Loss = 2.60956
2025-02-21 12:19:43.869437 Epoch 95  	Train Loss = 1.23405 Val Loss = 2.59946
2025-02-21 12:20:56.711353 Epoch 96  	Train Loss = 1.24288 Val Loss = 2.59600
2025-02-21 12:22:09.963180 Epoch 97  	Train Loss = 1.25290 Val Loss = 2.59648
2025-02-21 12:23:25.009744 Epoch 98  	Train Loss = 1.26381 Val Loss = 2.59030
2025-02-21 12:24:38.567440 Epoch 99  	Train Loss = 1.28111 Val Loss = 2.59318
2025-02-21 12:25:51.796346 Epoch 100  	Train Loss = 1.29727 Val Loss = 2.58571
2025-02-21 12:27:06.263868 Epoch 101  	Train Loss = 1.29974 Val Loss = 2.58599
2025-02-21 12:28:21.277543 Epoch 102  	Train Loss = 1.30879 Val Loss = 2.59132
2025-02-21 12:29:35.870736 Epoch 103  	Train Loss = 1.31372 Val Loss = 2.59762
2025-02-21 12:30:51.271525 Epoch 104  	Train Loss = 1.32256 Val Loss = 2.58970
2025-02-21 12:32:03.559864 Epoch 105  	Train Loss = 1.34840 Val Loss = 2.57879
2025-02-21 12:33:16.191878 Epoch 106  	Train Loss = 1.37225 Val Loss = 2.57568
2025-02-21 12:34:31.852002 Epoch 107  	Train Loss = 1.38416 Val Loss = 2.57701
2025-02-21 12:35:46.575222 Epoch 108  	Train Loss = 1.38782 Val Loss = 2.57553
2025-02-21 12:37:00.310305 Epoch 109  	Train Loss = 1.40978 Val Loss = 2.57645
2025-02-21 12:38:13.358863 Epoch 110  	Train Loss = 1.42561 Val Loss = 2.57486
2025-02-21 12:39:27.327902 Epoch 111  	Train Loss = 1.42715 Val Loss = 2.58180
2025-02-21 12:40:42.298838 Epoch 112  	Train Loss = 1.45941 Val Loss = 2.56604
2025-02-21 12:41:58.287483 Epoch 113  	Train Loss = 1.47144 Val Loss = 2.56399
2025-02-21 12:43:12.828371 Epoch 114  	Train Loss = 1.48934 Val Loss = 2.56873
2025-02-21 12:44:26.395608 Epoch 115  	Train Loss = 1.51927 Val Loss = 2.56104
2025-02-21 12:45:39.331463 Epoch 116  	Train Loss = 1.53404 Val Loss = 2.56382
2025-02-21 12:46:53.236422 Epoch 117  	Train Loss = 1.55162 Val Loss = 2.56725
2025-02-21 12:48:06.211561 Epoch 118  	Train Loss = 1.55881 Val Loss = 2.56842
2025-02-21 12:49:19.817245 Epoch 119  	Train Loss = 1.59492 Val Loss = 2.57756
2025-02-21 12:50:33.725471 Epoch 120  	Train Loss = 1.59649 Val Loss = 2.56721
2025-02-21 12:51:50.046850 Epoch 121  	Train Loss = 1.61915 Val Loss = 2.55617
2025-02-21 12:53:02.941031 Epoch 122  	Train Loss = 1.64834 Val Loss = 2.56230
2025-02-21 12:54:16.961983 Epoch 123  	Train Loss = 1.66772 Val Loss = 2.57600
2025-02-21 12:55:28.511485 Epoch 124  	Train Loss = 1.69714 Val Loss = 2.56779
2025-02-21 12:56:42.739120 Epoch 125  	Train Loss = 1.71153 Val Loss = 2.56823
2025-02-21 12:57:58.652415 Epoch 126  	Train Loss = 1.73400 Val Loss = 2.56312
2025-02-21 12:59:14.274851 Epoch 127  	Train Loss = 1.74176 Val Loss = 2.55688
2025-02-21 13:00:28.112410 Epoch 128  	Train Loss = 1.76505 Val Loss = 2.56033
2025-02-21 13:01:40.053329 Epoch 129  	Train Loss = 1.79036 Val Loss = 2.55566
2025-02-21 13:02:54.602126 Epoch 130  	Train Loss = 1.80212 Val Loss = 2.56916
2025-02-21 13:04:10.875038 Epoch 131  	Train Loss = 1.81810 Val Loss = 2.56192
2025-02-21 13:05:24.937623 Epoch 132  	Train Loss = 1.85069 Val Loss = 2.56385
2025-02-21 13:06:39.048505 Epoch 133  	Train Loss = 1.83826 Val Loss = 2.57302
2025-02-21 13:07:54.218172 Epoch 134  	Train Loss = 1.87845 Val Loss = 2.56497
2025-02-21 13:09:06.800065 Epoch 135  	Train Loss = 1.87801 Val Loss = 2.55010
2025-02-21 13:10:21.067476 Epoch 136  	Train Loss = 1.90056 Val Loss = 2.55524
2025-02-21 13:11:34.166846 Epoch 137  	Train Loss = 1.91443 Val Loss = 2.55106
2025-02-21 13:12:46.924011 Epoch 138  	Train Loss = 1.91893 Val Loss = 2.56328
2025-02-21 13:14:01.120732 Epoch 139  	Train Loss = 1.94569 Val Loss = 2.55564
2025-02-21 13:15:16.066882 Epoch 140  	Train Loss = 1.94988 Val Loss = 2.55229
2025-02-21 13:16:31.072945 Epoch 141  	Train Loss = 1.96832 Val Loss = 2.55109
2025-02-21 13:17:44.827839 Epoch 142  	Train Loss = 1.96633 Val Loss = 2.55721
2025-02-21 13:18:57.402592 Epoch 143  	Train Loss = 1.99402 Val Loss = 2.55673
2025-02-21 13:20:09.893144 Epoch 144  	Train Loss = 1.99191 Val Loss = 2.55376
2025-02-21 13:21:26.419001 Epoch 145  	Train Loss = 2.00181 Val Loss = 2.55386
2025-02-21 13:22:42.402612 Epoch 146  	Train Loss = 2.01476 Val Loss = 2.55501
2025-02-21 13:23:56.286445 Epoch 147  	Train Loss = 2.01407 Val Loss = 2.56870
2025-02-21 13:25:10.659436 Epoch 148  	Train Loss = 2.01741 Val Loss = 2.55965
2025-02-21 13:26:26.134701 Epoch 149  	Train Loss = 2.03518 Val Loss = 2.56558
2025-02-21 13:27:40.307205 Epoch 150  	Train Loss = 2.03858 Val Loss = 2.55994
2025-02-21 13:28:54.960541 Epoch 151  	Train Loss = 2.03538 Val Loss = 2.56686
2025-02-21 13:30:10.467995 Epoch 152  	Train Loss = 2.04557 Val Loss = 2.56092
2025-02-21 13:31:25.211879 Epoch 153  	Train Loss = 2.05324 Val Loss = 2.56554
2025-02-21 13:32:41.700791 Epoch 154  	Train Loss = 2.04769 Val Loss = 2.56459
2025-02-21 13:33:56.278132 Epoch 155  	Train Loss = 2.05117 Val Loss = 2.56074
Early stopping at epoch: 155
Best at epoch 135:
Train Loss = 1.87801
Train MAE = 2.16811, RMSE = 4.55831, MAPE = 5.14533
Val Loss = 2.55010
Val MAE = 2.55762, RMSE = 5.34528, MAPE = 6.68527
Model checkpoint saved to: ../saved_models/HimNet/HimNet-PEMSD7M-2025-02-21-10-21-59.pt
--------- Test ---------
All Steps (1-12) MAE = 2.53298, RMSE = 5.24819, MAPE = 6.36339
Step 1 MAE = 1.24480, RMSE = 2.15098, MAPE = 2.77118
Step 2 MAE = 1.73190, RMSE = 3.18469, MAPE = 3.97912
Step 3 MAE = 2.05881, RMSE = 3.93949, MAPE = 4.87314
Step 4 MAE = 2.30238, RMSE = 4.51322, MAPE = 5.59035
Step 5 MAE = 2.48827, RMSE = 4.96228, MAPE = 6.16569
Step 6 MAE = 2.63711, RMSE = 5.32329, MAPE = 6.63725
Step 7 MAE = 2.76207, RMSE = 5.62196, MAPE = 7.03312
Step 8 MAE = 2.86781, RMSE = 5.87581, MAPE = 7.36642
Step 9 MAE = 2.95974, RMSE = 6.08805, MAPE = 7.64634
Step 10 MAE = 3.04183, RMSE = 6.26864, MAPE = 7.89033
Step 11 MAE = 3.11544, RMSE = 6.42472, MAPE = 8.10456
Step 12 MAE = 3.18567, RMSE = 6.56762, MAPE = 8.30313
Inference time: 9.93 s
