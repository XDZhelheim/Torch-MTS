PEMS07
Trainset:	x-(16921, 12, 883, 3)	y-(16921, 12, 883, 3)
Valset:  	x-(5640, 12, 883, 3)  	y-(5640, 12, 883, 3)
Testset:	x-(5640, 12, 883, 3)	y-(5640, 12, 883, 3)

Random seed = 233
--------- HimNet ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "himnet",
    "lr": 0.001,
    "eps": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        40,
        60
    ],
    "clip_grad": 5,
    "batch_size": 16,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_nodes": 883,
        "input_dim": 3,
        "output_dim": 1,
        "tod_embedding_dim": 8,
        "dow_embedding_dim": 8,
        "out_steps": 12,
        "hidden_dim": 64,
        "num_layers": 1,
        "cheb_k": 2,
        "ycov_dim": 2,
        "node_embedding_dim": 16,
        "st_embedding_dim": 16,
        "tf_decay_steps": 6000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
HimNet                                   [16, 12, 883, 1]          14,128
├─Embedding: 1-1                         [16, 8]                   2,304
├─Embedding: 1-2                         [16, 8]                   56
├─HimEncoder: 1-3                        [16, 12, 883, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─HimGCRU: 3-1                 [16, 883, 64]             414,720
│    │    └─HimGCRU: 3-2                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-3                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-4                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-5                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-6                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-7                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-8                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-9                 [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-10                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-11                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-12                [16, 883, 64]             (recursive)
├─HimEncoder: 1-4                        [16, 12, 883, 64]         --
│    └─ModuleList: 2-2                   --                        --
│    │    └─HimGCRU: 3-13                [16, 883, 64]             414,720
│    │    └─HimGCRU: 3-14                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-15                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-16                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-17                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-18                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-19                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-20                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-21                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-22                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-23                [16, 883, 64]             (recursive)
│    │    └─HimGCRU: 3-24                [16, 883, 64]             (recursive)
├─Linear: 1-5                            [16, 883, 16]             1,040
├─HimDecoder: 1-6                        [16, 883, 64]             --
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-25                [16, 883, 64]             414,720
├─Linear: 1-7                            [16, 883, 1]              65
├─HimDecoder: 1-8                        [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-26                [16, 883, 64]             (recursive)
├─Linear: 1-9                            [16, 883, 1]              (recursive)
├─HimDecoder: 1-10                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-27                [16, 883, 64]             (recursive)
├─Linear: 1-11                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-12                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-28                [16, 883, 64]             (recursive)
├─Linear: 1-13                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-14                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-29                [16, 883, 64]             (recursive)
├─Linear: 1-15                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-16                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-30                [16, 883, 64]             (recursive)
├─Linear: 1-17                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-18                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-31                [16, 883, 64]             (recursive)
├─Linear: 1-19                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-20                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-32                [16, 883, 64]             (recursive)
├─Linear: 1-21                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-22                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-33                [16, 883, 64]             (recursive)
├─Linear: 1-23                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-24                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-34                [16, 883, 64]             (recursive)
├─Linear: 1-25                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-26                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-35                [16, 883, 64]             (recursive)
├─Linear: 1-27                           [16, 883, 1]              (recursive)
├─HimDecoder: 1-28                       [16, 883, 64]             (recursive)
│    └─ModuleList: 2-14                  --                        (recursive)
│    │    └─HimGCRU: 3-36                [16, 883, 64]             (recursive)
├─Linear: 1-29                           [16, 883, 1]              (recursive)
==========================================================================================
Total params: 1,261,753
Trainable params: 1,261,753
Non-trainable params: 0
Total mult-adds (G): 210.93
==========================================================================================
Input size (MB): 3.39
Forward/backward pass size (MB): 784.39
Params size (MB): 4.99
Estimated Total Size (MB): 792.77
==========================================================================================

Loss: HuberLoss

2025-02-21 10:45:50.139005 Epoch 1  	Train Loss = 22.99701 Val Loss = 39.11281
2025-02-21 11:08:12.989637 Epoch 2  	Train Loss = 18.46897 Val Loss = 35.00356
2025-02-21 11:32:59.139448 Epoch 3  	Train Loss = 17.98145 Val Loss = 30.60470
2025-02-21 11:55:47.492243 Epoch 4  	Train Loss = 17.73970 Val Loss = 24.97321
2025-02-21 12:17:54.993612 Epoch 5  	Train Loss = 17.49132 Val Loss = 25.41041
2025-02-21 12:40:03.758591 Epoch 6  	Train Loss = 17.25043 Val Loss = 24.10142
2025-02-21 13:03:15.381612 Epoch 7  	Train Loss = 17.05702 Val Loss = 34.80056
2025-02-21 13:25:49.716268 Epoch 8  	Train Loss = 16.87239 Val Loss = 23.52115
2025-02-21 13:47:54.724426 Epoch 9  	Train Loss = 16.61178 Val Loss = 24.71219
2025-02-21 14:09:33.937895 Epoch 10  	Train Loss = 16.34337 Val Loss = 24.09101
2025-02-21 14:31:42.006140 Epoch 11  	Train Loss = 16.10911 Val Loss = 23.00987
2025-02-21 14:53:45.310704 Epoch 12  	Train Loss = 15.89730 Val Loss = 23.50708
2025-02-21 15:15:45.568833 Epoch 13  	Train Loss = 15.75419 Val Loss = 21.59506
2025-02-21 15:37:37.366941 Epoch 14  	Train Loss = 15.63573 Val Loss = 21.80880
2025-02-21 16:01:02.948215 Epoch 15  	Train Loss = 15.52851 Val Loss = 22.37516
2025-02-21 16:24:24.784488 Epoch 16  	Train Loss = 15.44078 Val Loss = 21.85072
2025-02-21 16:49:02.827303 Epoch 17  	Train Loss = 15.37078 Val Loss = 21.05298
2025-02-21 17:11:29.211960 Epoch 18  	Train Loss = 15.28051 Val Loss = 22.10231
2025-02-21 17:33:15.033163 Epoch 19  	Train Loss = 15.22665 Val Loss = 21.76946
2025-02-21 17:55:23.778272 Epoch 20  	Train Loss = 15.17351 Val Loss = 21.62672
2025-02-21 18:16:54.959249 Epoch 21  	Train Loss = 15.13682 Val Loss = 20.94875
2025-02-21 18:38:38.684314 Epoch 22  	Train Loss = 15.06872 Val Loss = 21.15583
2025-02-21 19:00:02.250126 Epoch 23  	Train Loss = 15.04270 Val Loss = 20.50308
2025-02-21 19:21:12.588159 Epoch 24  	Train Loss = 14.99429 Val Loss = 20.52171
2025-02-21 19:42:33.612868 Epoch 25  	Train Loss = 14.97498 Val Loss = 21.46983
2025-02-21 20:04:16.328460 Epoch 26  	Train Loss = 14.95175 Val Loss = 20.60134
2025-02-21 20:25:26.080104 Epoch 27  	Train Loss = 14.93028 Val Loss = 21.67683
2025-02-21 20:46:46.251774 Epoch 28  	Train Loss = 14.89268 Val Loss = 21.28790
2025-02-21 21:08:15.830784 Epoch 29  	Train Loss = 14.88470 Val Loss = 21.00379
2025-02-21 21:29:31.080533 Epoch 30  	Train Loss = 14.85261 Val Loss = 20.40710
2025-02-21 21:51:38.625189 Epoch 31  	Train Loss = 14.87315 Val Loss = 20.03803
2025-02-21 22:13:00.818252 Epoch 32  	Train Loss = 14.85585 Val Loss = 20.51102
2025-02-21 22:34:55.523131 Epoch 33  	Train Loss = 14.85951 Val Loss = 21.95768
2025-02-21 22:57:09.716854 Epoch 34  	Train Loss = 14.86133 Val Loss = 20.63597
2025-02-21 23:18:47.415320 Epoch 35  	Train Loss = 14.85698 Val Loss = 20.72143
2025-02-21 23:39:56.991511 Epoch 36  	Train Loss = 14.86012 Val Loss = 20.37721
2025-02-22 00:01:21.262655 Epoch 37  	Train Loss = 14.90062 Val Loss = 20.65436
2025-02-22 00:23:27.240789 Epoch 38  	Train Loss = 14.90651 Val Loss = 20.26309
2025-02-22 00:45:12.108496 Epoch 39  	Train Loss = 14.92682 Val Loss = 20.56290
2025-02-22 01:07:23.771916 Epoch 40  	Train Loss = 15.01481 Val Loss = 19.88172
2025-02-22 01:30:13.301082 Epoch 41  	Train Loss = 14.66655 Val Loss = 19.06975
2025-02-22 01:53:10.814099 Epoch 42  	Train Loss = 14.66962 Val Loss = 18.94155
2025-02-22 02:15:29.795407 Epoch 43  	Train Loss = 14.70112 Val Loss = 18.92510
2025-02-22 02:37:22.478806 Epoch 44  	Train Loss = 14.76744 Val Loss = 18.95480
2025-02-22 02:59:51.402945 Epoch 45  	Train Loss = 14.87876 Val Loss = 18.90978
2025-02-22 03:24:06.351715 Epoch 46  	Train Loss = 14.92347 Val Loss = 19.06532
2025-02-22 03:48:25.330086 Epoch 47  	Train Loss = 14.99706 Val Loss = 18.93801
2025-02-22 04:11:42.464551 Epoch 48  	Train Loss = 15.13595 Val Loss = 18.93149
2025-02-22 04:34:25.242488 Epoch 49  	Train Loss = 15.22219 Val Loss = 18.87008
2025-02-22 04:57:37.302738 Epoch 50  	Train Loss = 15.32810 Val Loss = 18.82057
2025-02-22 05:19:37.292487 Epoch 51  	Train Loss = 15.45700 Val Loss = 18.85757
2025-02-22 05:41:40.689558 Epoch 52  	Train Loss = 15.58516 Val Loss = 18.86169
2025-02-22 06:05:45.633339 Epoch 53  	Train Loss = 15.68918 Val Loss = 18.81580
2025-02-22 06:27:45.946239 Epoch 54  	Train Loss = 15.82963 Val Loss = 18.74164
2025-02-22 06:49:17.359362 Epoch 55  	Train Loss = 15.94831 Val Loss = 18.80484
2025-02-22 07:10:39.697910 Epoch 56  	Train Loss = 16.09735 Val Loss = 18.64039
2025-02-22 07:33:18.439545 Epoch 57  	Train Loss = 16.20500 Val Loss = 18.73367
2025-02-22 07:57:50.777326 Epoch 58  	Train Loss = 16.33211 Val Loss = 18.68244
2025-02-22 08:21:45.649936 Epoch 59  	Train Loss = 16.44305 Val Loss = 18.64845
2025-02-22 08:45:27.398743 Epoch 60  	Train Loss = 16.50778 Val Loss = 18.62902
2025-02-22 09:08:53.385515 Epoch 61  	Train Loss = 16.48365 Val Loss = 18.55046
2025-02-22 09:31:02.862713 Epoch 62  	Train Loss = 16.54351 Val Loss = 18.56006
2025-02-22 09:54:01.817623 Epoch 63  	Train Loss = 16.63369 Val Loss = 18.57371
2025-02-22 10:16:53.700762 Epoch 64  	Train Loss = 16.70876 Val Loss = 18.56238
2025-02-22 10:39:30.452315 Epoch 65  	Train Loss = 16.74809 Val Loss = 18.55806
2025-02-22 11:01:57.599928 Epoch 66  	Train Loss = 16.80126 Val Loss = 18.56173
2025-02-22 11:25:31.993579 Epoch 67  	Train Loss = 16.83598 Val Loss = 18.56726
2025-02-22 11:48:55.444202 Epoch 68  	Train Loss = 16.86599 Val Loss = 18.55661
2025-02-22 12:12:14.406270 Epoch 69  	Train Loss = 16.90704 Val Loss = 18.56724
2025-02-22 12:35:41.502432 Epoch 70  	Train Loss = 16.91763 Val Loss = 18.58147
2025-02-22 12:58:23.249920 Epoch 71  	Train Loss = 16.93719 Val Loss = 18.58707
2025-02-22 13:22:04.326250 Epoch 72  	Train Loss = 16.97516 Val Loss = 18.56291
2025-02-22 13:45:09.182622 Epoch 73  	Train Loss = 16.97498 Val Loss = 18.58592
2025-02-22 14:08:01.551772 Epoch 74  	Train Loss = 16.98145 Val Loss = 18.56268
2025-02-22 14:31:53.898883 Epoch 75  	Train Loss = 16.97935 Val Loss = 18.57291
2025-02-22 14:55:36.493722 Epoch 76  	Train Loss = 16.99863 Val Loss = 18.58595
2025-02-22 15:19:13.521534 Epoch 77  	Train Loss = 17.01365 Val Loss = 18.59393
2025-02-22 15:42:18.034047 Epoch 78  	Train Loss = 17.01662 Val Loss = 18.57261
2025-02-22 16:05:18.926114 Epoch 79  	Train Loss = 17.01089 Val Loss = 18.55883
2025-02-22 16:28:19.189632 Epoch 80  	Train Loss = 17.01224 Val Loss = 18.56201
2025-02-22 16:51:39.385933 Epoch 81  	Train Loss = 17.01246 Val Loss = 18.59284
Early stopping at epoch: 81
Best at epoch 61:
Train Loss = 16.48365
Train MAE = 17.68940, RMSE = 30.34565, MAPE = 7.73947
Val Loss = 18.55046
Val MAE = 19.07509, RMSE = 32.57918, MAPE = 8.31646
Model checkpoint saved to: ../saved_models/HimNet/HimNet-PEMS07-2025-02-21-10-20-09.pt
--------- Test ---------
All Steps (1-12) MAE = 19.29632, RMSE = 32.77317, MAPE = 8.05080
Step 1 MAE = 16.06927, RMSE = 26.08997, MAPE = 6.72554
Step 2 MAE = 17.31835, RMSE = 28.59527, MAPE = 7.24576
Step 3 MAE = 17.99475, RMSE = 30.03203, MAPE = 7.51759
Step 4 MAE = 18.51740, RMSE = 31.11802, MAPE = 7.71989
Step 5 MAE = 18.96831, RMSE = 32.05109, MAPE = 7.90214
Step 6 MAE = 19.37466, RMSE = 32.86337, MAPE = 8.06858
Step 7 MAE = 19.75032, RMSE = 33.60520, MAPE = 8.22436
Step 8 MAE = 20.09151, RMSE = 34.27723, MAPE = 8.36471
Step 9 MAE = 20.41215, RMSE = 34.89360, MAPE = 8.50242
Step 10 MAE = 20.71192, RMSE = 35.45742, MAPE = 8.63813
Step 11 MAE = 21.01614, RMSE = 36.02409, MAPE = 8.77717
Step 12 MAE = 21.32808, RMSE = 36.55346, MAPE = 8.92202
Inference time: 201.77 s
