PEMSD7L
Trainset:	x-(7589, 12, 1026, 3)	y-(7589, 12, 1026, 3)
Valset:  	x-(2530, 12, 1026, 3)  	y-(2530, 12, 1026, 3)
Testset:	x-(2530, 12, 1026, 3)	y-(2530, 12, 1026, 3)

Random seed = 233
--------- STDN ---------
{
    "num_nodes": 1026,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "STDN",
    "pass_device": true,
    "lr": 0.001,
    "milestones": [
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 32,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_of_vertices": 1026,
        "adj_path": "../data/PEMSD7L/adj_PEMSD7L_distance.pkl",
        "L": 2,
        "K": 12,
        "d": 8,
        "node_miss_rate": 0.1,
        "T_miss_len": 12,
        "order": 3,
        "reference": 3,
        "time_slice_size": 5,
        "num_his": 12,
        "num_pred": 12,
        "in_channels": 1,
        "out_channels": 1,
        "bn_decay": 0.1,
        "device": "cuda:0"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
STDN                                                    [32, 12, 1026, 1]         1,109,376
├─FC: 1-1                                               [32, 12, 1026, 96]        --
│    └─ModuleList: 2-1                                  --                        --
│    │    └─conv2d_: 3-1                                [32, 12, 1026, 96]        384
│    │    └─conv2d_: 3-2                                [32, 12, 1026, 96]        9,504
├─gcn: 1-2                                              [32, 12, 1026, 96]        --
│    └─nconv: 2-2                                       [32, 96, 1026, 12]        --
│    └─nconv: 2-3                                       [32, 96, 1026, 12]        --
│    └─nconv: 2-4                                       [32, 96, 1026, 12]        --
│    └─FC: 2-5                                          [32, 12, 1026, 96]        --
│    │    └─ModuleList: 3-3                             --                        37,152
├─SEmbedding: 1-3                                       [32, 12, 1026, 96]        --
│    └─Linear: 2-6                                      [1026, 32]                1,056
│    └─LayerNorm: 2-7                                   [1026, 32]                --
│    └─LeakyReLU: 2-8                                   [1026, 32]                --
│    └─Linear: 2-9                                      [1026, 96]                3,168
│    └─LayerNorm: 2-10                                  [1026, 96]                --
├─TEmbedding: 1-4                                       [32, 12, 1026, 96]        --
│    └─FC: 2-11                                         [32, 24, 1026, 96]        --
│    │    └─ModuleList: 3-4                             --                        47,616
├─Trend: 1-5                                            [32, 12, 1026, 96]        --
├─Seasonal: 1-6                                         [32, 12, 1026, 96]        --
├─FeedForward: 1-7                                      [32, 12, 1026, 96]        --
│    └─ModuleList: 2-12                                 --                        --
│    │    └─Linear: 3-5                                 [32, 12, 1026, 96]        9,312
│    └─LayerNorm: 2-13                                  [32, 12, 1026, 96]        --
├─FeedForward: 1-8                                      [32, 12, 1026, 96]        --
│    └─ModuleList: 2-14                                 --                        --
│    │    └─Linear: 3-6                                 [32, 12, 1026, 96]        9,312
│    └─LayerNorm: 2-15                                  [32, 12, 1026, 96]        --
├─GRUEncoder: 1-9                                       [32, 12, 1026, 96]        --
│    └─ModuleList: 2-16                                 --                        --
│    │    └─GRU: 3-7                                    [32, 1026, 96]            55,584
│    │    └─GRU: 3-8                                    [32, 1026, 96]            55,584
│    │    └─GRU: 3-9                                    [32, 1026, 96]            55,584
│    │    └─GRU: 3-10                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-11                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-12                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-13                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-14                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-15                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-16                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-17                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-18                                   [32, 1026, 96]            55,584
├─GRUEncoder: 1-10                                      [32, 12, 1026, 96]        --
│    └─ModuleList: 2-17                                 --                        --
│    │    └─GRU: 3-19                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-20                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-21                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-22                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-23                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-24                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-25                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-26                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-27                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-28                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-29                                   [32, 1026, 96]            55,584
│    │    └─GRU: 3-30                                   [32, 1026, 96]            55,584
├─ModuleList: 1-11                                      --                        --
│    └─AttentionDecoder: 2-18                           [32, 12, 1026, 96]        886,464
│    │    └─MAB_new: 3-31                               [32, 3, 1026, 288]        112,320
│    │    └─MAB_new: 3-32                               [32, 12, 1026, 96]        93,312
│    └─AttentionDecoder: 2-19                           [32, 12, 1026, 96]        886,464
│    │    └─MAB_new: 3-33                               [32, 3, 1026, 288]        112,320
│    │    └─MAB_new: 3-34                               [32, 12, 1026, 96]        93,312
├─FC: 1-12                                              [32, 12, 1026, 1]         --
│    └─ModuleList: 2-20                                 --                        --
│    │    └─conv2d_: 3-35                               [32, 12, 1026, 96]        9,504
│    │    └─conv2d_: 3-36                               [32, 12, 1026, 1]         99
=========================================================================================================
Total params: 4,754,691
Trainable params: 4,754,691
Non-trainable params: 0
Total mult-adds (G): 153.95
=========================================================================================================
Input size (MB): 7.88
Forward/backward pass size (MB): 15136.34
Params size (MB): 7.49
Estimated Total Size (MB): 15151.71
=========================================================================================================

Loss: MaskedMAELoss

2025-02-20 09:45:18.860953 Epoch 1  	Train Loss = 3.80555 Val Loss = 3.13569
2025-02-20 09:48:01.188270 Epoch 2  	Train Loss = 3.09651 Val Loss = 2.97142
2025-02-20 09:50:43.440010 Epoch 3  	Train Loss = 2.99851 Val Loss = 2.94678
2025-02-20 09:53:25.840626 Epoch 4  	Train Loss = 2.91790 Val Loss = 3.02498
2025-02-20 09:56:08.390164 Epoch 5  	Train Loss = 2.87731 Val Loss = 3.10782
2025-02-20 09:58:50.725260 Epoch 6  	Train Loss = 2.81980 Val Loss = 2.94368
2025-02-20 10:01:33.169908 Epoch 7  	Train Loss = 2.84985 Val Loss = 3.31907
2025-02-20 10:04:15.685324 Epoch 8  	Train Loss = 2.77944 Val Loss = 2.91044
2025-02-20 10:06:58.314585 Epoch 9  	Train Loss = 2.76929 Val Loss = 2.82129
2025-02-20 10:09:40.594085 Epoch 10  	Train Loss = 2.71569 Val Loss = 3.05096
2025-02-20 10:12:22.579525 Epoch 11  	Train Loss = 2.68068 Val Loss = 3.02886
2025-02-20 10:15:04.746682 Epoch 12  	Train Loss = 2.67320 Val Loss = 2.83902
2025-02-20 10:17:46.712217 Epoch 13  	Train Loss = 2.72302 Val Loss = 2.85567
2025-02-20 10:20:28.714946 Epoch 14  	Train Loss = 2.70076 Val Loss = 2.94613
2025-02-20 10:23:10.634504 Epoch 15  	Train Loss = 2.67284 Val Loss = 3.05856
2025-02-20 10:25:53.117154 Epoch 16  	Train Loss = 2.64464 Val Loss = 2.83896
2025-02-20 10:28:35.109787 Epoch 17  	Train Loss = 2.64939 Val Loss = 2.87245
2025-02-20 10:31:17.015614 Epoch 18  	Train Loss = 2.63232 Val Loss = 2.89313
2025-02-20 10:33:59.265927 Epoch 19  	Train Loss = 2.63968 Val Loss = 3.09782
2025-02-20 10:36:41.408934 Epoch 20  	Train Loss = 2.65258 Val Loss = 3.00182
2025-02-20 10:39:23.605988 Epoch 21  	Train Loss = 2.64564 Val Loss = 2.82149
2025-02-20 10:42:05.912067 Epoch 22  	Train Loss = 2.63063 Val Loss = 2.98421
2025-02-20 10:44:48.197863 Epoch 23  	Train Loss = 2.62515 Val Loss = 2.89938
2025-02-20 10:47:30.658034 Epoch 24  	Train Loss = 2.57617 Val Loss = 3.11288
2025-02-20 10:50:12.970188 Epoch 25  	Train Loss = 2.59974 Val Loss = 2.90851
2025-02-20 10:52:55.080825 Epoch 26  	Train Loss = 2.59324 Val Loss = 2.92326
2025-02-20 10:55:37.509473 Epoch 27  	Train Loss = 2.60897 Val Loss = 2.89637
2025-02-20 10:58:19.667629 Epoch 28  	Train Loss = 2.56972 Val Loss = 3.03063
2025-02-20 11:01:01.944574 Epoch 29  	Train Loss = 2.55757 Val Loss = 3.03728
Early stopping at epoch: 29
Best at epoch 9:
Train Loss = 2.76929
Train MAE = 2.41806, RMSE = 4.97021, MAPE = 5.93454
Val Loss = 2.82129
Val MAE = 2.83601, RMSE = 5.85031, MAPE = 7.57229
Model checkpoint saved to: ../saved_models/STDN/STDN-PEMSD7L-2025-02-20-09-42-29.pt
--------- Test ---------
All Steps (1-12) MAE = 2.84898, RMSE = 5.88504, MAPE = 7.30037
Step 1 MAE = 1.50796, RMSE = 2.59925, MAPE = 3.51540
Step 2 MAE = 1.99044, RMSE = 3.67594, MAPE = 4.73759
Step 3 MAE = 2.34069, RMSE = 4.50945, MAPE = 5.71604
Step 4 MAE = 2.60090, RMSE = 5.14862, MAPE = 6.49905
Step 5 MAE = 2.80256, RMSE = 5.63932, MAPE = 7.12556
Step 6 MAE = 2.96294, RMSE = 6.02907, MAPE = 7.62516
Step 7 MAE = 3.09881, RMSE = 6.34110, MAPE = 8.04325
Step 8 MAE = 3.20979, RMSE = 6.58680, MAPE = 8.37417
Step 9 MAE = 3.30156, RMSE = 6.78621, MAPE = 8.65436
Step 10 MAE = 3.38733, RMSE = 6.95787, MAPE = 8.90291
Step 11 MAE = 3.45954, RMSE = 7.09327, MAPE = 9.11131
Step 12 MAE = 3.52526, RMSE = 7.20882, MAPE = 9.29964
Inference time: 16.85 s
