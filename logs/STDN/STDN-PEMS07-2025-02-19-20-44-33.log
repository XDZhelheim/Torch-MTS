PEMS07
Trainset:	x-(16921, 12, 883, 3)	y-(16921, 12, 883, 3)
Valset:  	x-(5640, 12, 883, 3)  	y-(5640, 12, 883, 3)
Testset:	x-(5640, 12, 883, 3)	y-(5640, 12, 883, 3)

Random seed = 233
--------- STDN ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "STDN",
    "pass_device": true,
    "lr": 0.001,
    "milestones": [
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 32,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_of_vertices": 883,
        "adj_path": "../data/PEMS07/adj_PEMS07.pkl",
        "L": 2,
        "K": 12,
        "d": 8,
        "node_miss_rate": 0.1,
        "T_miss_len": 12,
        "order": 3,
        "reference": 3,
        "time_slice_size": 5,
        "num_his": 12,
        "num_pred": 12,
        "in_channels": 1,
        "out_channels": 1,
        "bn_decay": 0.1,
        "device": "cuda:0"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
STDN                                                    [32, 12, 883, 1]          1,081,920
├─FC: 1-1                                               [32, 12, 883, 96]         --
│    └─ModuleList: 2-1                                  --                        --
│    │    └─conv2d_: 3-1                                [32, 12, 883, 96]         384
│    │    └─conv2d_: 3-2                                [32, 12, 883, 96]         9,504
├─gcn: 1-2                                              [32, 12, 883, 96]         --
│    └─nconv: 2-2                                       [32, 96, 883, 12]         --
│    └─nconv: 2-3                                       [32, 96, 883, 12]         --
│    └─nconv: 2-4                                       [32, 96, 883, 12]         --
│    └─FC: 2-5                                          [32, 12, 883, 96]         --
│    │    └─ModuleList: 3-3                             --                        37,152
├─SEmbedding: 1-3                                       [32, 12, 883, 96]         --
│    └─Linear: 2-6                                      [883, 32]                 1,056
│    └─LayerNorm: 2-7                                   [883, 32]                 --
│    └─LeakyReLU: 2-8                                   [883, 32]                 --
│    └─Linear: 2-9                                      [883, 96]                 3,168
│    └─LayerNorm: 2-10                                  [883, 96]                 --
├─TEmbedding: 1-4                                       [32, 12, 883, 96]         --
│    └─FC: 2-11                                         [32, 24, 883, 96]         --
│    │    └─ModuleList: 3-4                             --                        47,616
├─Trend: 1-5                                            [32, 12, 883, 96]         --
├─Seasonal: 1-6                                         [32, 12, 883, 96]         --
├─FeedForward: 1-7                                      [32, 12, 883, 96]         --
│    └─ModuleList: 2-12                                 --                        --
│    │    └─Linear: 3-5                                 [32, 12, 883, 96]         9,312
│    └─LayerNorm: 2-13                                  [32, 12, 883, 96]         --
├─FeedForward: 1-8                                      [32, 12, 883, 96]         --
│    └─ModuleList: 2-14                                 --                        --
│    │    └─Linear: 3-6                                 [32, 12, 883, 96]         9,312
│    └─LayerNorm: 2-15                                  [32, 12, 883, 96]         --
├─GRUEncoder: 1-9                                       [32, 12, 883, 96]         --
│    └─ModuleList: 2-16                                 --                        --
│    │    └─GRU: 3-7                                    [32, 883, 96]             55,584
│    │    └─GRU: 3-8                                    [32, 883, 96]             55,584
│    │    └─GRU: 3-9                                    [32, 883, 96]             55,584
│    │    └─GRU: 3-10                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-11                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-12                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-13                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-14                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-15                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-16                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-17                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-18                                   [32, 883, 96]             55,584
├─GRUEncoder: 1-10                                      [32, 12, 883, 96]         --
│    └─ModuleList: 2-17                                 --                        --
│    │    └─GRU: 3-19                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-20                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-21                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-22                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-23                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-24                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-25                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-26                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-27                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-28                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-29                                   [32, 883, 96]             55,584
│    │    └─GRU: 3-30                                   [32, 883, 96]             55,584
├─ModuleList: 1-11                                      --                        --
│    └─AttentionDecoder: 2-18                           [32, 12, 883, 96]         762,912
│    │    └─MAB_new: 3-31                               [32, 3, 883, 288]         112,320
│    │    └─MAB_new: 3-32                               [32, 12, 883, 96]         93,312
│    └─AttentionDecoder: 2-19                           [32, 12, 883, 96]         762,912
│    │    └─MAB_new: 3-33                               [32, 3, 883, 288]         112,320
│    │    └─MAB_new: 3-34                               [32, 12, 883, 96]         93,312
├─FC: 1-12                                              [32, 12, 883, 1]          --
│    └─ModuleList: 2-20                                 --                        --
│    │    └─conv2d_: 3-35                               [32, 12, 883, 96]         9,504
│    │    └─conv2d_: 3-36                               [32, 12, 883, 1]          99
=========================================================================================================
Total params: 4,480,131
Trainable params: 4,480,131
Non-trainable params: 0
Total mult-adds (G): 132.50
=========================================================================================================
Input size (MB): 6.78
Forward/backward pass size (MB): 13026.69
Params size (MB): 7.49
Estimated Total Size (MB): 13040.97
=========================================================================================================

Loss: HuberLoss

2025-02-19 20:50:00.020876 Epoch 1  	Train Loss = 34.96528 Val Loss = 23.66920
2025-02-19 20:55:10.415721 Epoch 2  	Train Loss = 30.75334 Val Loss = 22.67642
2025-02-19 21:00:20.990185 Epoch 3  	Train Loss = 30.05155 Val Loss = 22.68868
2025-02-19 21:05:31.271889 Epoch 4  	Train Loss = 29.32358 Val Loss = 22.10458
2025-02-19 21:10:41.952177 Epoch 5  	Train Loss = 28.19588 Val Loss = 21.09304
2025-02-19 21:15:52.392874 Epoch 6  	Train Loss = 28.39718 Val Loss = 21.27174
2025-02-19 21:22:06.345469 Epoch 7  	Train Loss = 28.72152 Val Loss = 21.33042
2025-02-19 21:28:50.151633 Epoch 8  	Train Loss = 28.60318 Val Loss = 22.37311
2025-02-19 21:36:51.488506 Epoch 9  	Train Loss = 28.02299 Val Loss = 22.03255
2025-02-19 21:43:46.765606 Epoch 10  	Train Loss = 27.43761 Val Loss = 21.17651
2025-02-19 21:54:20.941416 Epoch 11  	Train Loss = 28.41320 Val Loss = 22.06146
2025-02-19 22:04:55.752847 Epoch 12  	Train Loss = 28.00356 Val Loss = 19.96851
2025-02-19 22:13:46.353098 Epoch 13  	Train Loss = 28.42934 Val Loss = 20.70638
2025-02-19 22:23:49.022439 Epoch 14  	Train Loss = 27.63085 Val Loss = 20.89504
2025-02-19 22:34:20.088153 Epoch 15  	Train Loss = 27.70058 Val Loss = 20.11790
2025-02-19 22:43:52.866331 Epoch 16  	Train Loss = 27.45624 Val Loss = 20.50793
2025-02-19 22:49:12.508714 Epoch 17  	Train Loss = 27.76419 Val Loss = 20.18553
2025-02-19 22:54:22.240596 Epoch 18  	Train Loss = 27.58671 Val Loss = 20.17226
2025-02-19 22:59:31.778073 Epoch 19  	Train Loss = 27.58851 Val Loss = 20.57044
2025-02-19 23:04:41.634913 Epoch 20  	Train Loss = 27.79023 Val Loss = 20.39889
2025-02-19 23:09:51.293136 Epoch 21  	Train Loss = 27.01993 Val Loss = 22.45103
2025-02-19 23:15:01.060873 Epoch 22  	Train Loss = 27.27426 Val Loss = 21.00720
2025-02-19 23:20:10.905627 Epoch 23  	Train Loss = 25.83818 Val Loss = 21.18813
2025-02-19 23:25:20.975204 Epoch 24  	Train Loss = 26.53419 Val Loss = 21.72518
2025-02-19 23:30:31.274381 Epoch 25  	Train Loss = 27.56947 Val Loss = 21.11286
2025-02-19 23:35:41.160981 Epoch 26  	Train Loss = 26.63654 Val Loss = 20.47676
2025-02-19 23:40:50.846622 Epoch 27  	Train Loss = 27.11235 Val Loss = 20.52264
2025-02-19 23:46:00.252649 Epoch 28  	Train Loss = 26.13253 Val Loss = 21.03820
2025-02-19 23:51:09.631042 Epoch 29  	Train Loss = 26.64805 Val Loss = 20.73265
2025-02-19 23:56:19.008264 Epoch 30  	Train Loss = 25.81733 Val Loss = 20.90079
2025-02-20 00:01:28.343308 Epoch 31  	Train Loss = 25.85623 Val Loss = 19.95631
2025-02-20 00:06:37.855000 Epoch 32  	Train Loss = 26.09629 Val Loss = 20.13428
2025-02-20 00:11:47.258644 Epoch 33  	Train Loss = 25.48190 Val Loss = 20.33121
2025-02-20 00:16:56.667932 Epoch 34  	Train Loss = 25.57271 Val Loss = 20.09364
2025-02-20 00:22:06.425528 Epoch 35  	Train Loss = 25.75606 Val Loss = 20.09883
2025-02-20 00:27:16.178474 Epoch 36  	Train Loss = 25.80624 Val Loss = 20.88000
2025-02-20 00:32:26.150208 Epoch 37  	Train Loss = 25.49330 Val Loss = 20.25121
2025-02-20 00:37:35.688997 Epoch 38  	Train Loss = 26.36902 Val Loss = 20.30563
2025-02-20 00:42:44.880153 Epoch 39  	Train Loss = 25.89502 Val Loss = 20.18557
2025-02-20 00:47:54.628672 Epoch 40  	Train Loss = 25.61855 Val Loss = 21.26258
2025-02-20 00:53:05.457614 Epoch 41  	Train Loss = 25.89782 Val Loss = 20.57630
2025-02-20 00:58:16.602317 Epoch 42  	Train Loss = 26.30803 Val Loss = 21.22112
2025-02-20 01:03:29.597057 Epoch 43  	Train Loss = 26.03695 Val Loss = 20.46456
2025-02-20 01:08:40.639854 Epoch 44  	Train Loss = 26.18452 Val Loss = 20.36797
2025-02-20 01:13:49.368040 Epoch 45  	Train Loss = 25.79300 Val Loss = 20.11581
2025-02-20 01:18:57.944441 Epoch 46  	Train Loss = 26.06572 Val Loss = 20.34741
2025-02-20 01:24:06.509035 Epoch 47  	Train Loss = 25.15104 Val Loss = 21.12548
2025-02-20 01:29:14.870551 Epoch 48  	Train Loss = 25.65463 Val Loss = 20.27458
2025-02-20 01:34:23.169289 Epoch 49  	Train Loss = 26.00253 Val Loss = 20.17400
2025-02-20 01:39:31.365196 Epoch 50  	Train Loss = 25.62828 Val Loss = 21.66525
2025-02-20 01:44:39.715290 Epoch 51  	Train Loss = 26.07816 Val Loss = 20.47579
Early stopping at epoch: 51
Best at epoch 31:
Train Loss = 25.85623
Train MAE = 18.97081, RMSE = 32.15875, MAPE = 10.95045
Val Loss = 19.95631
Val MAE = 20.46299, RMSE = 35.23433, MAPE = 11.64799
Model checkpoint saved to: ../saved_models/STDN/STDN-PEMS07-2025-02-19-20-44-33.pt
--------- Test ---------
All Steps (1-12) MAE = 20.65336, RMSE = 34.77425, MAPE = 10.98411
Step 1 MAE = 18.38522, RMSE = 29.77913, MAPE = 10.20140
Step 2 MAE = 19.06768, RMSE = 31.39300, MAPE = 10.51124
Step 3 MAE = 19.59704, RMSE = 32.53302, MAPE = 10.82622
Step 4 MAE = 20.04307, RMSE = 33.42609, MAPE = 11.03815
Step 5 MAE = 20.39333, RMSE = 34.16475, MAPE = 11.18093
Step 6 MAE = 20.73101, RMSE = 34.83541, MAPE = 11.25341
Step 7 MAE = 21.00287, RMSE = 35.40549, MAPE = 11.24016
Step 8 MAE = 21.25677, RMSE = 35.94554, MAPE = 11.11542
Step 9 MAE = 21.49654, RMSE = 36.46815, MAPE = 11.03337
Step 10 MAE = 21.73166, RMSE = 36.96502, MAPE = 11.19127
Step 11 MAE = 21.94759, RMSE = 37.45159, MAPE = 11.17581
Step 12 MAE = 22.18450, RMSE = 37.91359, MAPE = 11.04065
Inference time: 32.56 s
