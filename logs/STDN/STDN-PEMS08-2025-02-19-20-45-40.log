PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 3)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 3)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 3)

Random seed = 233
--------- STDN ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "STDN",
    "pass_device": true,
    "lr": 0.001,
    "milestones": [
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_of_vertices": 170,
        "adj_path": "../data/PEMS08/adj_PEMS08.pkl",
        "L": 2,
        "K": 16,
        "d": 8,
        "node_miss_rate": 0.1,
        "T_miss_len": 12,
        "order": 3,
        "reference": 3,
        "time_slice_size": 5,
        "num_his": 12,
        "num_pred": 12,
        "in_channels": 1,
        "out_channels": 1,
        "bn_decay": 0.1,
        "device": "cuda:0"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
STDN                                                    [64, 12, 170, 1]          2,177,536
├─FC: 1-1                                               [64, 12, 170, 128]        --
│    └─ModuleList: 2-1                                  --                        --
│    │    └─conv2d_: 3-1                                [64, 12, 170, 128]        512
│    │    └─conv2d_: 3-2                                [64, 12, 170, 128]        16,768
├─gcn: 1-2                                              [64, 12, 170, 128]        --
│    └─nconv: 2-2                                       [64, 128, 170, 12]        --
│    └─nconv: 2-3                                       [64, 128, 170, 12]        --
│    └─nconv: 2-4                                       [64, 128, 170, 12]        --
│    └─FC: 2-5                                          [64, 12, 170, 128]        --
│    │    └─ModuleList: 3-3                             --                        65,920
├─SEmbedding: 1-3                                       [64, 12, 170, 128]        --
│    └─Linear: 2-6                                      [170, 32]                 1,056
│    └─LayerNorm: 2-7                                   [170, 32]                 --
│    └─LeakyReLU: 2-8                                   [170, 32]                 --
│    └─Linear: 2-9                                      [170, 128]                4,224
│    └─LayerNorm: 2-10                                  [170, 128]                --
├─TEmbedding: 1-4                                       [64, 12, 170, 128]        --
│    └─FC: 2-11                                         [64, 24, 170, 128]        --
│    │    └─ModuleList: 3-4                             --                        71,680
├─Trend: 1-5                                            [64, 12, 170, 128]        --
├─Seasonal: 1-6                                         [64, 12, 170, 128]        --
├─FeedForward: 1-7                                      [64, 12, 170, 128]        --
│    └─ModuleList: 2-12                                 --                        --
│    │    └─Linear: 3-5                                 [64, 12, 170, 128]        16,512
│    └─LayerNorm: 2-13                                  [64, 12, 170, 128]        --
├─FeedForward: 1-8                                      [64, 12, 170, 128]        --
│    └─ModuleList: 2-14                                 --                        --
│    │    └─Linear: 3-6                                 [64, 12, 170, 128]        16,512
│    └─LayerNorm: 2-15                                  [64, 12, 170, 128]        --
├─GRUEncoder: 1-9                                       [64, 12, 170, 128]        --
│    └─ModuleList: 2-16                                 --                        --
│    │    └─GRU: 3-7                                    [64, 170, 128]            98,688
│    │    └─GRU: 3-8                                    [64, 170, 128]            98,688
│    │    └─GRU: 3-9                                    [64, 170, 128]            98,688
│    │    └─GRU: 3-10                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-11                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-12                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-13                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-14                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-15                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-16                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-17                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-18                                   [64, 170, 128]            98,688
├─GRUEncoder: 1-10                                      [64, 12, 170, 128]        --
│    └─ModuleList: 2-17                                 --                        --
│    │    └─GRU: 3-19                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-20                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-21                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-22                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-23                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-24                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-25                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-26                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-27                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-28                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-29                                   [64, 170, 128]            98,688
│    │    └─GRU: 3-30                                   [64, 170, 128]            98,688
├─ModuleList: 1-11                                      --                        --
│    └─AttentionDecoder: 2-18                           [64, 12, 170, 128]        195,840
│    │    └─MAB_new: 3-31                               [64, 3, 170, 384]         198,912
│    │    └─MAB_new: 3-32                               [64, 12, 170, 128]        165,376
│    └─AttentionDecoder: 2-19                           [64, 12, 170, 128]        195,840
│    │    └─MAB_new: 3-33                               [64, 3, 170, 384]         198,912
│    │    └─MAB_new: 3-34                               [64, 12, 170, 128]        165,376
├─FC: 1-12                                              [64, 12, 170, 1]          --
│    └─ModuleList: 2-20                                 --                        --
│    │    └─conv2d_: 3-35                               [64, 12, 170, 128]        16,768
│    │    └─conv2d_: 3-36                               [64, 12, 170, 1]          131
=========================================================================================================
Total params: 5,876,387
Trainable params: 5,876,387
Non-trainable params: 0
Total mult-adds (G): 87.41
=========================================================================================================
Input size (MB): 2.61
Forward/backward pass size (MB): 6686.98
Params size (MB): 13.23
Estimated Total Size (MB): 6702.82
=========================================================================================================

Loss: HuberLoss

2025-02-19 20:46:29.580496 Epoch 1  	Train Loss = 25.55868 Val Loss = 17.80722
2025-02-19 20:47:15.438846 Epoch 2  	Train Loss = 19.91402 Val Loss = 16.29757
2025-02-19 20:48:01.543737 Epoch 3  	Train Loss = 19.23380 Val Loss = 16.32756
2025-02-19 20:48:47.753673 Epoch 4  	Train Loss = 18.16898 Val Loss = 15.34633
2025-02-19 20:49:33.838122 Epoch 5  	Train Loss = 17.75197 Val Loss = 15.93764
2025-02-19 20:50:20.189521 Epoch 6  	Train Loss = 18.02744 Val Loss = 15.60066
2025-02-19 20:51:06.391590 Epoch 7  	Train Loss = 17.25308 Val Loss = 15.32003
2025-02-19 20:51:52.790517 Epoch 8  	Train Loss = 17.56871 Val Loss = 15.30112
2025-02-19 20:52:39.082580 Epoch 9  	Train Loss = 17.01848 Val Loss = 14.97657
2025-02-19 20:53:25.339623 Epoch 10  	Train Loss = 16.57406 Val Loss = 14.65975
2025-02-19 20:54:11.838476 Epoch 11  	Train Loss = 16.90833 Val Loss = 14.93648
2025-02-19 20:54:57.839605 Epoch 12  	Train Loss = 16.29028 Val Loss = 15.18795
2025-02-19 20:55:44.150429 Epoch 13  	Train Loss = 16.56547 Val Loss = 14.56369
2025-02-19 20:56:30.484000 Epoch 14  	Train Loss = 16.64955 Val Loss = 15.70626
2025-02-19 20:57:16.691106 Epoch 15  	Train Loss = 16.96592 Val Loss = 14.22975
2025-02-19 20:58:02.934486 Epoch 16  	Train Loss = 16.84609 Val Loss = 14.08119
2025-02-19 20:58:48.921823 Epoch 17  	Train Loss = 16.06666 Val Loss = 14.88084
2025-02-19 20:59:35.198150 Epoch 18  	Train Loss = 16.17650 Val Loss = 14.02131
2025-02-19 21:00:21.460142 Epoch 19  	Train Loss = 16.96430 Val Loss = 14.70289
2025-02-19 21:01:07.532570 Epoch 20  	Train Loss = 15.98918 Val Loss = 14.24343
2025-02-19 21:01:53.873840 Epoch 21  	Train Loss = 15.82488 Val Loss = 15.20761
2025-02-19 21:02:40.158201 Epoch 22  	Train Loss = 16.39919 Val Loss = 15.17247
2025-02-19 21:03:26.333633 Epoch 23  	Train Loss = 16.29520 Val Loss = 14.14602
2025-02-19 21:04:12.321349 Epoch 24  	Train Loss = 16.34963 Val Loss = 16.55143
2025-02-19 21:04:58.028179 Epoch 25  	Train Loss = 15.85916 Val Loss = 14.58015
2025-02-19 21:05:44.108512 Epoch 26  	Train Loss = 15.89268 Val Loss = 13.95579
2025-02-19 21:06:30.387360 Epoch 27  	Train Loss = 16.01095 Val Loss = 14.17121
2025-02-19 21:07:16.789547 Epoch 28  	Train Loss = 16.10015 Val Loss = 14.97072
2025-02-19 21:08:03.140866 Epoch 29  	Train Loss = 16.10013 Val Loss = 14.37443
2025-02-19 21:08:48.982189 Epoch 30  	Train Loss = 15.46800 Val Loss = 14.58023
2025-02-19 21:09:35.053574 Epoch 31  	Train Loss = 15.53197 Val Loss = 13.77015
2025-02-19 21:10:21.193930 Epoch 32  	Train Loss = 15.29159 Val Loss = 13.79992
2025-02-19 21:11:07.337240 Epoch 33  	Train Loss = 14.56906 Val Loss = 14.57361
2025-02-19 21:11:53.577120 Epoch 34  	Train Loss = 15.24812 Val Loss = 14.08562
2025-02-19 21:12:39.922807 Epoch 35  	Train Loss = 15.22716 Val Loss = 13.94180
2025-02-19 21:13:26.297079 Epoch 36  	Train Loss = 15.44413 Val Loss = 14.05953
2025-02-19 21:14:12.466543 Epoch 37  	Train Loss = 15.05753 Val Loss = 14.19192
2025-02-19 21:15:04.261156 Epoch 38  	Train Loss = 15.04843 Val Loss = 13.94035
2025-02-19 21:16:26.870639 Epoch 39  	Train Loss = 15.72574 Val Loss = 13.74781
2025-02-19 21:17:47.412946 Epoch 40  	Train Loss = 15.20402 Val Loss = 14.04063
2025-02-19 21:19:09.132226 Epoch 41  	Train Loss = 15.07992 Val Loss = 14.15369
2025-02-19 21:20:08.299460 Epoch 42  	Train Loss = 15.23791 Val Loss = 14.03266
2025-02-19 21:21:04.554211 Epoch 43  	Train Loss = 15.79290 Val Loss = 14.06990
2025-02-19 21:21:59.166977 Epoch 44  	Train Loss = 15.68484 Val Loss = 14.17110
2025-02-19 21:22:53.173985 Epoch 45  	Train Loss = 14.93420 Val Loss = 14.40342
2025-02-19 21:23:47.642386 Epoch 46  	Train Loss = 15.36020 Val Loss = 13.99122
2025-02-19 21:24:41.886784 Epoch 47  	Train Loss = 15.48542 Val Loss = 14.84413
2025-02-19 21:25:38.463175 Epoch 48  	Train Loss = 15.38933 Val Loss = 14.57716
2025-02-19 21:26:30.809358 Epoch 49  	Train Loss = 15.19327 Val Loss = 14.10083
2025-02-19 21:27:24.152074 Epoch 50  	Train Loss = 15.58342 Val Loss = 14.54682
2025-02-19 21:28:22.618006 Epoch 51  	Train Loss = 15.72178 Val Loss = 14.09124
2025-02-19 21:29:21.115114 Epoch 52  	Train Loss = 14.96164 Val Loss = 14.25768
2025-02-19 21:30:19.523824 Epoch 53  	Train Loss = 15.05306 Val Loss = 14.24706
2025-02-19 21:31:18.009851 Epoch 54  	Train Loss = 16.12395 Val Loss = 13.86720
2025-02-19 21:32:16.274893 Epoch 55  	Train Loss = 15.15161 Val Loss = 14.05178
2025-02-19 21:33:14.790641 Epoch 56  	Train Loss = 15.25447 Val Loss = 13.90884
2025-02-19 21:34:07.970631 Epoch 57  	Train Loss = 15.40032 Val Loss = 13.99137
2025-02-19 21:35:03.573342 Epoch 58  	Train Loss = 15.42081 Val Loss = 13.99318
2025-02-19 21:36:01.658822 Epoch 59  	Train Loss = 15.36197 Val Loss = 14.08752
Early stopping at epoch: 59
Best at epoch 39:
Train Loss = 15.72574
Train MAE = 12.21688, RMSE = 20.92036, MAPE = 9.32315
Val Loss = 13.74781
Val MAE = 14.16726, RMSE = 24.96716, MAPE = 12.52856
Model checkpoint saved to: ../saved_models/STDN/STDN-PEMS08-2025-02-19-20-45-40.pt
--------- Test ---------
All Steps (1-12) MAE = 14.25184, RMSE = 24.39132, MAPE = 10.66793
Step 1 MAE = 12.80660, RMSE = 20.70263, MAPE = 10.26749
Step 2 MAE = 13.16023, RMSE = 21.67795, MAPE = 10.45184
Step 3 MAE = 13.49028, RMSE = 22.52707, MAPE = 10.60246
Step 4 MAE = 13.77696, RMSE = 23.26843, MAPE = 10.68439
Step 5 MAE = 14.01667, RMSE = 23.91101, MAPE = 10.66729
Step 6 MAE = 14.22672, RMSE = 24.44020, MAPE = 10.66382
Step 7 MAE = 14.44919, RMSE = 24.92128, MAPE = 10.75619
Step 8 MAE = 14.63844, RMSE = 25.34793, MAPE = 10.74914
Step 9 MAE = 14.83503, RMSE = 25.75314, MAPE = 10.72391
Step 10 MAE = 15.02367, RMSE = 26.12131, MAPE = 10.78192
Step 11 MAE = 15.19767, RMSE = 26.44085, MAPE = 10.81377
Step 12 MAE = 15.40071, RMSE = 26.73029, MAPE = 10.85296
Inference time: 5.71 s
