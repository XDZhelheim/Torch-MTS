PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 3)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 3)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 3)

Random seed = 233
--------- STDN ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "STDN",
    "loss": "huber",
    "pass_device": true,
    "lr": 0.001,
    "milestones": [
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_of_vertices": 307,
        "adj_path": "../data/PEMS04/adj_PEMS04.pkl",
        "L": 2,
        "K": 16,
        "d": 8,
        "node_miss_rate": 0.1,
        "T_miss_len": 12,
        "order": 3,
        "reference": 3,
        "time_slice_size": 5,
        "num_his": 12,
        "num_pred": 12,
        "in_channels": 1,
        "out_channels": 1,
        "bn_decay": 0.1,
        "device": "cuda:0"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
STDN                                                    [64, 12, 307, 1]          2,212,608
├─FC: 1-1                                               [64, 12, 307, 128]        --
│    └─ModuleList: 2-1                                  --                        --
│    │    └─conv2d_: 3-1                                [64, 12, 307, 128]        512
│    │    └─conv2d_: 3-2                                [64, 12, 307, 128]        16,768
├─gcn: 1-2                                              [64, 12, 307, 128]        --
│    └─nconv: 2-2                                       [64, 128, 307, 12]        --
│    └─nconv: 2-3                                       [64, 128, 307, 12]        --
│    └─nconv: 2-4                                       [64, 128, 307, 12]        --
│    └─FC: 2-5                                          [64, 12, 307, 128]        --
│    │    └─ModuleList: 3-3                             --                        65,920
├─SEmbedding: 1-3                                       [64, 12, 307, 128]        --
│    └─Linear: 2-6                                      [307, 32]                 1,056
│    └─LayerNorm: 2-7                                   [307, 32]                 --
│    └─LeakyReLU: 2-8                                   [307, 32]                 --
│    └─Linear: 2-9                                      [307, 128]                4,224
│    └─LayerNorm: 2-10                                  [307, 128]                --
├─TEmbedding: 1-4                                       [64, 12, 307, 128]        --
│    └─FC: 2-11                                         [64, 24, 307, 128]        --
│    │    └─ModuleList: 3-4                             --                        71,680
├─Trend: 1-5                                            [64, 12, 307, 128]        --
├─Seasonal: 1-6                                         [64, 12, 307, 128]        --
├─FeedForward: 1-7                                      [64, 12, 307, 128]        --
│    └─ModuleList: 2-12                                 --                        --
│    │    └─Linear: 3-5                                 [64, 12, 307, 128]        16,512
│    └─LayerNorm: 2-13                                  [64, 12, 307, 128]        --
├─FeedForward: 1-8                                      [64, 12, 307, 128]        --
│    └─ModuleList: 2-14                                 --                        --
│    │    └─Linear: 3-6                                 [64, 12, 307, 128]        16,512
│    └─LayerNorm: 2-15                                  [64, 12, 307, 128]        --
├─GRUEncoder: 1-9                                       [64, 12, 307, 128]        --
│    └─ModuleList: 2-16                                 --                        --
│    │    └─GRU: 3-7                                    [64, 307, 128]            98,688
│    │    └─GRU: 3-8                                    [64, 307, 128]            98,688
│    │    └─GRU: 3-9                                    [64, 307, 128]            98,688
│    │    └─GRU: 3-10                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-11                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-12                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-13                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-14                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-15                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-16                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-17                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-18                                   [64, 307, 128]            98,688
├─GRUEncoder: 1-10                                      [64, 12, 307, 128]        --
│    └─ModuleList: 2-17                                 --                        --
│    │    └─GRU: 3-19                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-20                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-21                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-22                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-23                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-24                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-25                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-26                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-27                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-28                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-29                                   [64, 307, 128]            98,688
│    │    └─GRU: 3-30                                   [64, 307, 128]            98,688
├─ModuleList: 1-11                                      --                        --
│    └─AttentionDecoder: 2-18                           [64, 12, 307, 128]        353,664
│    │    └─MAB_new: 3-31                               [64, 3, 307, 384]         198,912
│    │    └─MAB_new: 3-32                               [64, 12, 307, 128]        165,376
│    └─AttentionDecoder: 2-19                           [64, 12, 307, 128]        353,664
│    │    └─MAB_new: 3-33                               [64, 3, 307, 384]         198,912
│    │    └─MAB_new: 3-34                               [64, 12, 307, 128]        165,376
├─FC: 1-12                                              [64, 12, 307, 1]          --
│    └─ModuleList: 2-20                                 --                        --
│    │    └─conv2d_: 3-35                               [64, 12, 307, 128]        16,768
│    │    └─conv2d_: 3-36                               [64, 12, 307, 1]          131
=========================================================================================================
Total params: 6,227,107
Trainable params: 6,227,107
Non-trainable params: 0
Total mult-adds (G): 157.72
=========================================================================================================
Input size (MB): 4.72
Forward/backward pass size (MB): 12075.90
Params size (MB): 13.23
Estimated Total Size (MB): 12093.84
=========================================================================================================

Loss: HuberLoss

2025-02-19 19:35:15.564853 Epoch 1  	Train Loss = 29.77900 Val Loss = 22.24271
2025-02-19 19:36:35.587473 Epoch 2  	Train Loss = 24.22952 Val Loss = 21.27365
2025-02-19 19:37:55.496258 Epoch 3  	Train Loss = 23.14139 Val Loss = 20.14557
2025-02-19 19:39:15.408433 Epoch 4  	Train Loss = 22.70027 Val Loss = 19.68994
2025-02-19 19:40:35.344185 Epoch 5  	Train Loss = 21.89431 Val Loss = 19.47708
2025-02-19 19:41:55.496673 Epoch 6  	Train Loss = 22.05746 Val Loss = 20.50254
2025-02-19 19:43:15.536515 Epoch 7  	Train Loss = 22.37630 Val Loss = 19.43856
2025-02-19 19:44:35.512487 Epoch 8  	Train Loss = 21.53519 Val Loss = 20.72004
2025-02-19 19:45:55.564709 Epoch 9  	Train Loss = 21.20199 Val Loss = 18.91756
2025-02-19 19:47:15.518495 Epoch 10  	Train Loss = 21.23170 Val Loss = 21.08182
2025-02-19 19:48:35.649855 Epoch 11  	Train Loss = 21.45268 Val Loss = 18.62167
2025-02-19 19:49:55.641287 Epoch 12  	Train Loss = 21.58728 Val Loss = 19.35825
2025-02-19 19:51:15.914660 Epoch 13  	Train Loss = 21.07625 Val Loss = 19.25701
2025-02-19 19:52:36.086358 Epoch 14  	Train Loss = 21.50326 Val Loss = 19.22681
2025-02-19 19:53:56.132315 Epoch 15  	Train Loss = 21.46500 Val Loss = 18.76700
2025-02-19 19:55:16.038934 Epoch 16  	Train Loss = 20.73543 Val Loss = 19.52949
2025-02-19 19:56:35.987249 Epoch 17  	Train Loss = 21.07485 Val Loss = 18.88254
2025-02-19 19:57:56.089017 Epoch 18  	Train Loss = 21.35205 Val Loss = 19.34239
2025-02-19 19:59:16.070576 Epoch 19  	Train Loss = 21.10503 Val Loss = 19.32127
2025-02-19 20:00:36.062622 Epoch 20  	Train Loss = 21.15905 Val Loss = 19.42540
2025-02-19 20:01:56.164905 Epoch 21  	Train Loss = 20.83094 Val Loss = 18.82049
2025-02-19 20:03:16.127604 Epoch 22  	Train Loss = 21.14548 Val Loss = 19.22947
2025-02-19 20:04:36.061244 Epoch 23  	Train Loss = 20.99820 Val Loss = 19.81327
2025-02-19 20:05:55.894047 Epoch 24  	Train Loss = 21.11673 Val Loss = 19.29785
2025-02-19 20:07:15.804219 Epoch 25  	Train Loss = 20.18054 Val Loss = 18.25488
2025-02-19 20:08:35.858011 Epoch 26  	Train Loss = 20.77949 Val Loss = 18.33517
2025-02-19 20:09:55.798123 Epoch 27  	Train Loss = 20.23795 Val Loss = 19.63772
2025-02-19 20:11:15.784243 Epoch 28  	Train Loss = 20.71665 Val Loss = 18.94485
2025-02-19 20:12:35.780085 Epoch 29  	Train Loss = 20.34965 Val Loss = 18.84049
2025-02-19 20:13:55.826954 Epoch 30  	Train Loss = 20.41360 Val Loss = 18.37014
2025-02-19 20:15:15.969644 Epoch 31  	Train Loss = 20.72254 Val Loss = 17.63726
2025-02-19 20:16:35.979383 Epoch 32  	Train Loss = 20.52397 Val Loss = 18.40253
2025-02-19 20:17:55.903890 Epoch 33  	Train Loss = 20.60574 Val Loss = 19.28307
2025-02-19 20:19:16.035913 Epoch 34  	Train Loss = 19.84050 Val Loss = 18.07905
2025-02-19 20:20:36.088268 Epoch 35  	Train Loss = 20.68709 Val Loss = 18.67731
2025-02-19 20:21:56.116984 Epoch 36  	Train Loss = 19.90007 Val Loss = 18.67595
2025-02-19 20:23:16.168062 Epoch 37  	Train Loss = 19.79185 Val Loss = 18.02036
2025-02-19 20:24:36.201448 Epoch 38  	Train Loss = 20.37134 Val Loss = 20.08863
2025-02-19 20:25:56.011236 Epoch 39  	Train Loss = 19.96076 Val Loss = 18.35042
2025-02-19 20:27:15.749785 Epoch 40  	Train Loss = 20.24030 Val Loss = 18.24235
2025-02-19 20:28:35.698427 Epoch 41  	Train Loss = 19.98678 Val Loss = 17.73094
2025-02-19 20:29:55.646424 Epoch 42  	Train Loss = 19.94396 Val Loss = 18.07093
2025-02-19 20:31:15.454492 Epoch 43  	Train Loss = 19.66200 Val Loss = 18.12269
2025-02-19 20:32:35.225966 Epoch 44  	Train Loss = 19.81861 Val Loss = 17.66221
2025-02-19 20:33:54.995534 Epoch 45  	Train Loss = 19.60232 Val Loss = 18.23842
2025-02-19 20:35:14.777442 Epoch 46  	Train Loss = 19.23660 Val Loss = 17.83968
2025-02-19 20:36:34.505562 Epoch 47  	Train Loss = 20.26471 Val Loss = 18.95742
2025-02-19 20:37:54.266472 Epoch 48  	Train Loss = 20.22850 Val Loss = 17.93149
2025-02-19 20:39:14.189409 Epoch 49  	Train Loss = 20.11236 Val Loss = 19.03526
2025-02-19 20:40:34.108940 Epoch 50  	Train Loss = 20.23943 Val Loss = 19.05135
2025-02-19 20:41:53.818316 Epoch 51  	Train Loss = 19.79582 Val Loss = 18.69031
Early stopping at epoch: 51
Best at epoch 31:
Train Loss = 20.72254
Train MAE = 16.99391, RMSE = 28.08833, MAPE = 12.70251
Val Loss = 17.63726
Val MAE = 18.40308, RMSE = 30.69268, MAPE = 12.21755
Model checkpoint saved to: ../saved_models/STDN/STDN-PEMS04-2025-02-19-19-33-51.pt
--------- Test ---------
All Steps (1-12) MAE = 18.40475, RMSE = 30.22027, MAPE = 12.50318
Step 1 MAE = 16.97363, RMSE = 27.49848, MAPE = 11.77116
Step 2 MAE = 17.33580, RMSE = 28.25348, MAPE = 12.00169
Step 3 MAE = 17.66245, RMSE = 28.87752, MAPE = 12.15187
Step 4 MAE = 17.93892, RMSE = 29.40389, MAPE = 12.28501
Step 5 MAE = 18.17760, RMSE = 29.84391, MAPE = 12.37055
Step 6 MAE = 18.38540, RMSE = 30.22836, MAPE = 12.44122
Step 7 MAE = 18.58119, RMSE = 30.57272, MAPE = 12.51777
Step 8 MAE = 18.77049, RMSE = 30.90224, MAPE = 12.64323
Step 9 MAE = 18.95262, RMSE = 31.20923, MAPE = 12.73477
Step 10 MAE = 19.14283, RMSE = 31.50530, MAPE = 12.88381
Step 11 MAE = 19.35010, RMSE = 31.81502, MAPE = 13.04837
Step 12 MAE = 19.58594, RMSE = 32.14615, MAPE = 13.18860
Inference time: 7.98 s
