PEMS03
Trainset:	x-(15711, 12, 358, 3)	y-(15711, 12, 358, 3)
Valset:  	x-(5237, 12, 358, 3)  	y-(5237, 12, 358, 3)
Testset:	x-(5237, 12, 358, 3)	y-(5237, 12, 358, 3)

Random seed = 233
--------- STDN ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "STDN",
    "pass_device": true,
    "lr": 0.001,
    "milestones": [
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_of_vertices": 358,
        "adj_path": "../data/PEMS03/adj_PEMS03.pkl",
        "L": 2,
        "K": 16,
        "d": 8,
        "node_miss_rate": 0.1,
        "T_miss_len": 12,
        "order": 3,
        "reference": 3,
        "time_slice_size": 5,
        "num_his": 12,
        "num_pred": 12,
        "in_channels": 1,
        "out_channels": 1,
        "bn_decay": 0.1,
        "device": "cuda:0"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
STDN                                                    [64, 12, 358, 1]          2,225,664
├─FC: 1-1                                               [64, 12, 358, 128]        --
│    └─ModuleList: 2-1                                  --                        --
│    │    └─conv2d_: 3-1                                [64, 12, 358, 128]        512
│    │    └─conv2d_: 3-2                                [64, 12, 358, 128]        16,768
├─gcn: 1-2                                              [64, 12, 358, 128]        --
│    └─nconv: 2-2                                       [64, 128, 358, 12]        --
│    └─nconv: 2-3                                       [64, 128, 358, 12]        --
│    └─nconv: 2-4                                       [64, 128, 358, 12]        --
│    └─FC: 2-5                                          [64, 12, 358, 128]        --
│    │    └─ModuleList: 3-3                             --                        65,920
├─SEmbedding: 1-3                                       [64, 12, 358, 128]        --
│    └─Linear: 2-6                                      [358, 32]                 1,056
│    └─LayerNorm: 2-7                                   [358, 32]                 --
│    └─LeakyReLU: 2-8                                   [358, 32]                 --
│    └─Linear: 2-9                                      [358, 128]                4,224
│    └─LayerNorm: 2-10                                  [358, 128]                --
├─TEmbedding: 1-4                                       [64, 12, 358, 128]        --
│    └─FC: 2-11                                         [64, 24, 358, 128]        --
│    │    └─ModuleList: 3-4                             --                        71,680
├─Trend: 1-5                                            [64, 12, 358, 128]        --
├─Seasonal: 1-6                                         [64, 12, 358, 128]        --
├─FeedForward: 1-7                                      [64, 12, 358, 128]        --
│    └─ModuleList: 2-12                                 --                        --
│    │    └─Linear: 3-5                                 [64, 12, 358, 128]        16,512
│    └─LayerNorm: 2-13                                  [64, 12, 358, 128]        --
├─FeedForward: 1-8                                      [64, 12, 358, 128]        --
│    └─ModuleList: 2-14                                 --                        --
│    │    └─Linear: 3-6                                 [64, 12, 358, 128]        16,512
│    └─LayerNorm: 2-15                                  [64, 12, 358, 128]        --
├─GRUEncoder: 1-9                                       [64, 12, 358, 128]        --
│    └─ModuleList: 2-16                                 --                        --
│    │    └─GRU: 3-7                                    [64, 358, 128]            98,688
│    │    └─GRU: 3-8                                    [64, 358, 128]            98,688
│    │    └─GRU: 3-9                                    [64, 358, 128]            98,688
│    │    └─GRU: 3-10                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-11                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-12                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-13                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-14                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-15                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-16                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-17                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-18                                   [64, 358, 128]            98,688
├─GRUEncoder: 1-10                                      [64, 12, 358, 128]        --
│    └─ModuleList: 2-17                                 --                        --
│    │    └─GRU: 3-19                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-20                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-21                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-22                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-23                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-24                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-25                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-26                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-27                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-28                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-29                                   [64, 358, 128]            98,688
│    │    └─GRU: 3-30                                   [64, 358, 128]            98,688
├─ModuleList: 1-11                                      --                        --
│    └─AttentionDecoder: 2-18                           [64, 12, 358, 128]        412,416
│    │    └─MAB_new: 3-31                               [64, 3, 358, 384]         198,912
│    │    └─MAB_new: 3-32                               [64, 12, 358, 128]        165,376
│    └─AttentionDecoder: 2-19                           [64, 12, 358, 128]        412,416
│    │    └─MAB_new: 3-33                               [64, 3, 358, 384]         198,912
│    │    └─MAB_new: 3-34                               [64, 12, 358, 128]        165,376
├─FC: 1-12                                              [64, 12, 358, 1]          --
│    └─ModuleList: 2-20                                 --                        --
│    │    └─conv2d_: 3-35                               [64, 12, 358, 128]        16,768
│    │    └─conv2d_: 3-36                               [64, 12, 358, 1]          131
=========================================================================================================
Total params: 6,357,667
Trainable params: 6,357,667
Non-trainable params: 0
Total mult-adds (G): 183.90
=========================================================================================================
Input size (MB): 5.50
Forward/backward pass size (MB): 14081.99
Params size (MB): 13.23
Estimated Total Size (MB): 14100.72
=========================================================================================================

Loss: HuberLoss

2025-02-20 09:41:21.000268 Epoch 1  	Train Loss = 22.55817 Val Loss = 16.20202
2025-02-20 09:43:45.534104 Epoch 2  	Train Loss = 18.31601 Val Loss = 14.86904
2025-02-20 09:46:10.114682 Epoch 3  	Train Loss = 17.38231 Val Loss = 14.35478
2025-02-20 09:48:34.676024 Epoch 4  	Train Loss = 17.28415 Val Loss = 14.00778
2025-02-20 09:50:59.152772 Epoch 5  	Train Loss = 16.39825 Val Loss = 14.13656
2025-02-20 09:53:23.731442 Epoch 6  	Train Loss = 16.67630 Val Loss = 13.89191
2025-02-20 09:55:48.409545 Epoch 7  	Train Loss = 16.43762 Val Loss = 14.08249
2025-02-20 09:58:12.844575 Epoch 8  	Train Loss = 16.57023 Val Loss = 13.69206
2025-02-20 10:00:37.329894 Epoch 9  	Train Loss = 16.59225 Val Loss = 13.91720
2025-02-20 10:03:02.232453 Epoch 10  	Train Loss = 16.09799 Val Loss = 13.73466
2025-02-20 10:05:26.949768 Epoch 11  	Train Loss = 16.23296 Val Loss = 13.62848
2025-02-20 10:07:51.507886 Epoch 12  	Train Loss = 16.15117 Val Loss = 14.13208
2025-02-20 10:10:15.909592 Epoch 13  	Train Loss = 16.70276 Val Loss = 13.58988
2025-02-20 10:12:40.353770 Epoch 14  	Train Loss = 15.98943 Val Loss = 14.32891
2025-02-20 10:15:04.747888 Epoch 15  	Train Loss = 15.79183 Val Loss = 13.57567
2025-02-20 10:17:29.443697 Epoch 16  	Train Loss = 15.93444 Val Loss = 13.50712
2025-02-20 10:19:53.913604 Epoch 17  	Train Loss = 15.48042 Val Loss = 13.78976
2025-02-20 10:22:18.369649 Epoch 18  	Train Loss = 15.31040 Val Loss = 13.63039
2025-02-20 10:24:42.813436 Epoch 19  	Train Loss = 15.74827 Val Loss = 13.45123
2025-02-20 10:27:07.238240 Epoch 20  	Train Loss = 15.60094 Val Loss = 13.65900
2025-02-20 10:29:31.636759 Epoch 21  	Train Loss = 15.56432 Val Loss = 13.35650
2025-02-20 10:31:55.830122 Epoch 22  	Train Loss = 15.92590 Val Loss = 13.47904
2025-02-20 10:34:20.432507 Epoch 23  	Train Loss = 15.63930 Val Loss = 13.74077
2025-02-20 10:36:44.700633 Epoch 24  	Train Loss = 15.29506 Val Loss = 13.75145
2025-02-20 10:39:08.969107 Epoch 25  	Train Loss = 16.05523 Val Loss = 13.40329
2025-02-20 10:41:33.218733 Epoch 26  	Train Loss = 16.23195 Val Loss = 13.70280
2025-02-20 10:43:57.465260 Epoch 27  	Train Loss = 15.76397 Val Loss = 13.41141
2025-02-20 10:46:21.722828 Epoch 28  	Train Loss = 15.40934 Val Loss = 13.46970
2025-02-20 10:48:46.007992 Epoch 29  	Train Loss = 15.90850 Val Loss = 13.95113
2025-02-20 10:51:10.341217 Epoch 30  	Train Loss = 15.63189 Val Loss = 13.32709
2025-02-20 10:53:34.838805 Epoch 31  	Train Loss = 15.29104 Val Loss = 14.59185
2025-02-20 10:55:59.243113 Epoch 32  	Train Loss = 15.52861 Val Loss = 13.18267
2025-02-20 10:58:23.704525 Epoch 33  	Train Loss = 15.15208 Val Loss = 13.18834
2025-02-20 11:00:48.114852 Epoch 34  	Train Loss = 15.07714 Val Loss = 13.40805
2025-02-20 11:03:12.547837 Epoch 35  	Train Loss = 15.53130 Val Loss = 13.82181
2025-02-20 11:05:36.965555 Epoch 36  	Train Loss = 15.41207 Val Loss = 13.34824
2025-02-20 11:08:01.344896 Epoch 37  	Train Loss = 15.29426 Val Loss = 13.32554
2025-02-20 11:10:25.674218 Epoch 38  	Train Loss = 15.29936 Val Loss = 13.86422
2025-02-20 11:12:50.028231 Epoch 39  	Train Loss = 15.44003 Val Loss = 13.21483
2025-02-20 11:15:14.353085 Epoch 40  	Train Loss = 14.83363 Val Loss = 13.19096
2025-02-20 11:17:38.632378 Epoch 41  	Train Loss = 15.29463 Val Loss = 13.52755
2025-02-20 11:20:02.926960 Epoch 42  	Train Loss = 14.89627 Val Loss = 13.56938
2025-02-20 11:22:27.304885 Epoch 43  	Train Loss = 14.99368 Val Loss = 13.33416
2025-02-20 11:24:51.655902 Epoch 44  	Train Loss = 15.18701 Val Loss = 13.89567
2025-02-20 11:27:15.950631 Epoch 45  	Train Loss = 15.05746 Val Loss = 13.95774
2025-02-20 11:29:40.302421 Epoch 46  	Train Loss = 14.95624 Val Loss = 13.28871
2025-02-20 11:32:04.682146 Epoch 47  	Train Loss = 15.25126 Val Loss = 13.19418
2025-02-20 11:34:29.026537 Epoch 48  	Train Loss = 14.79845 Val Loss = 13.54492
2025-02-20 11:36:53.361007 Epoch 49  	Train Loss = 15.06806 Val Loss = 13.30535
2025-02-20 11:39:17.756827 Epoch 50  	Train Loss = 15.00619 Val Loss = 13.57838
2025-02-20 11:41:42.166764 Epoch 51  	Train Loss = 14.99484 Val Loss = 13.40078
2025-02-20 11:44:06.693861 Epoch 52  	Train Loss = 15.00692 Val Loss = 13.33976
Early stopping at epoch: 52
Best at epoch 32:
Train Loss = 15.52861
Train MAE = 12.27509, RMSE = 20.19568, MAPE = 11.76442
Val Loss = 13.18267
Val MAE = 13.69566, RMSE = 22.22008, MAPE = 13.48666
Model checkpoint saved to: ../saved_models/STDN/STDN-PEMS03-2025-02-20-09-38-49.pt
--------- Test ---------
All Steps (1-12) MAE = 15.53829, RMSE = 27.28141, MAPE = 16.86485
Step 1 MAE = 13.31562, RMSE = 23.41063, MAPE = 15.14097
Step 2 MAE = 13.85776, RMSE = 24.44561, MAPE = 15.34997
Step 3 MAE = 14.34950, RMSE = 25.31127, MAPE = 15.69615
Step 4 MAE = 14.78905, RMSE = 26.07455, MAPE = 16.06984
Step 5 MAE = 15.17226, RMSE = 26.70168, MAPE = 16.51245
Step 6 MAE = 15.52209, RMSE = 27.24132, MAPE = 16.75169
Step 7 MAE = 15.86462, RMSE = 27.77100, MAPE = 17.23154
Step 8 MAE = 16.17715, RMSE = 28.25830, MAPE = 17.49061
Step 9 MAE = 16.46017, RMSE = 28.69944, MAPE = 17.76505
Step 10 MAE = 16.72281, RMSE = 29.12286, MAPE = 17.89544
Step 11 MAE = 16.98063, RMSE = 29.52936, MAPE = 18.07260
Step 12 MAE = 17.24765, RMSE = 29.94165, MAPE = 18.40167
Inference time: 14.44 s
