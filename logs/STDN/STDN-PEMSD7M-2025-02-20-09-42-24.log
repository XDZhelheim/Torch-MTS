PEMSD7M
Trainset:	x-(7589, 12, 228, 3)	y-(7589, 12, 228, 3)
Valset:  	x-(2530, 12, 228, 3)  	y-(2530, 12, 228, 3)
Testset:	x-(2530, 12, 228, 3)	y-(2530, 12, 228, 3)

Random seed = 233
--------- STDN ---------
{
    "num_nodes": 228,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "y_time_of_day": true,
    "y_day_of_week": true,
    "runner": "STDN",
    "pass_device": true,
    "lr": 0.001,
    "milestones": [
        30,
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "model_args": {
        "num_of_vertices": 228,
        "adj_path": "../data/PEMSD7M/adj_PEMSD7M_distance.pkl",
        "L": 2,
        "K": 16,
        "d": 8,
        "node_miss_rate": 0.1,
        "T_miss_len": 12,
        "order": 3,
        "reference": 3,
        "time_slice_size": 5,
        "num_his": 12,
        "num_pred": 12,
        "in_channels": 1,
        "out_channels": 1,
        "bn_decay": 0.1,
        "device": "cuda:0"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
STDN                                                    [64, 12, 228, 1]          2,192,384
├─FC: 1-1                                               [64, 12, 228, 128]        --
│    └─ModuleList: 2-1                                  --                        --
│    │    └─conv2d_: 3-1                                [64, 12, 228, 128]        512
│    │    └─conv2d_: 3-2                                [64, 12, 228, 128]        16,768
├─gcn: 1-2                                              [64, 12, 228, 128]        --
│    └─nconv: 2-2                                       [64, 128, 228, 12]        --
│    └─nconv: 2-3                                       [64, 128, 228, 12]        --
│    └─nconv: 2-4                                       [64, 128, 228, 12]        --
│    └─FC: 2-5                                          [64, 12, 228, 128]        --
│    │    └─ModuleList: 3-3                             --                        65,920
├─SEmbedding: 1-3                                       [64, 12, 228, 128]        --
│    └─Linear: 2-6                                      [228, 32]                 1,056
│    └─LayerNorm: 2-7                                   [228, 32]                 --
│    └─LeakyReLU: 2-8                                   [228, 32]                 --
│    └─Linear: 2-9                                      [228, 128]                4,224
│    └─LayerNorm: 2-10                                  [228, 128]                --
├─TEmbedding: 1-4                                       [64, 12, 228, 128]        --
│    └─FC: 2-11                                         [64, 24, 228, 128]        --
│    │    └─ModuleList: 3-4                             --                        71,680
├─Trend: 1-5                                            [64, 12, 228, 128]        --
├─Seasonal: 1-6                                         [64, 12, 228, 128]        --
├─FeedForward: 1-7                                      [64, 12, 228, 128]        --
│    └─ModuleList: 2-12                                 --                        --
│    │    └─Linear: 3-5                                 [64, 12, 228, 128]        16,512
│    └─LayerNorm: 2-13                                  [64, 12, 228, 128]        --
├─FeedForward: 1-8                                      [64, 12, 228, 128]        --
│    └─ModuleList: 2-14                                 --                        --
│    │    └─Linear: 3-6                                 [64, 12, 228, 128]        16,512
│    └─LayerNorm: 2-15                                  [64, 12, 228, 128]        --
├─GRUEncoder: 1-9                                       [64, 12, 228, 128]        --
│    └─ModuleList: 2-16                                 --                        --
│    │    └─GRU: 3-7                                    [64, 228, 128]            98,688
│    │    └─GRU: 3-8                                    [64, 228, 128]            98,688
│    │    └─GRU: 3-9                                    [64, 228, 128]            98,688
│    │    └─GRU: 3-10                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-11                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-12                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-13                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-14                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-15                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-16                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-17                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-18                                   [64, 228, 128]            98,688
├─GRUEncoder: 1-10                                      [64, 12, 228, 128]        --
│    └─ModuleList: 2-17                                 --                        --
│    │    └─GRU: 3-19                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-20                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-21                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-22                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-23                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-24                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-25                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-26                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-27                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-28                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-29                                   [64, 228, 128]            98,688
│    │    └─GRU: 3-30                                   [64, 228, 128]            98,688
├─ModuleList: 1-11                                      --                        --
│    └─AttentionDecoder: 2-18                           [64, 12, 228, 128]        262,656
│    │    └─MAB_new: 3-31                               [64, 3, 228, 384]         198,912
│    │    └─MAB_new: 3-32                               [64, 12, 228, 128]        165,376
│    └─AttentionDecoder: 2-19                           [64, 12, 228, 128]        262,656
│    │    └─MAB_new: 3-33                               [64, 3, 228, 384]         198,912
│    │    └─MAB_new: 3-34                               [64, 12, 228, 128]        165,376
├─FC: 1-12                                              [64, 12, 228, 1]          --
│    └─ModuleList: 2-20                                 --                        --
│    │    └─conv2d_: 3-35                               [64, 12, 228, 128]        16,768
│    │    └─conv2d_: 3-36                               [64, 12, 228, 1]          131
=========================================================================================================
Total params: 6,024,867
Trainable params: 6,024,867
Non-trainable params: 0
Total mult-adds (G): 117.18
=========================================================================================================
Input size (MB): 3.50
Forward/backward pass size (MB): 8968.42
Params size (MB): 13.23
Estimated Total Size (MB): 8985.15
=========================================================================================================

Loss: MaskedMAELoss

2025-02-20 09:43:12.082390 Epoch 1  	Train Loss = 3.55577 Val Loss = 3.08318
2025-02-20 09:43:56.125524 Epoch 2  	Train Loss = 2.83584 Val Loss = 2.81318
2025-02-20 09:44:40.202021 Epoch 3  	Train Loss = 2.69496 Val Loss = 2.78037
2025-02-20 09:45:24.288591 Epoch 4  	Train Loss = 2.60435 Val Loss = 2.74243
2025-02-20 09:46:08.367458 Epoch 5  	Train Loss = 2.53544 Val Loss = 2.96015
2025-02-20 09:46:52.423297 Epoch 6  	Train Loss = 2.46771 Val Loss = 2.67738
2025-02-20 09:47:36.563544 Epoch 7  	Train Loss = 2.41788 Val Loss = 2.83947
2025-02-20 09:48:20.612442 Epoch 8  	Train Loss = 2.41925 Val Loss = 2.77849
2025-02-20 09:49:04.669255 Epoch 9  	Train Loss = 2.40534 Val Loss = 2.64918
2025-02-20 09:49:48.991632 Epoch 10  	Train Loss = 2.34446 Val Loss = 2.87249
2025-02-20 09:50:33.050204 Epoch 11  	Train Loss = 2.37024 Val Loss = 2.66769
2025-02-20 09:51:17.069153 Epoch 12  	Train Loss = 2.35460 Val Loss = 2.69825
2025-02-20 09:52:01.147359 Epoch 13  	Train Loss = 2.31775 Val Loss = 2.76112
2025-02-20 09:52:45.203585 Epoch 14  	Train Loss = 2.32661 Val Loss = 2.94439
2025-02-20 09:53:29.251908 Epoch 15  	Train Loss = 2.32062 Val Loss = 2.67253
2025-02-20 09:54:13.521560 Epoch 16  	Train Loss = 2.26672 Val Loss = 2.68814
2025-02-20 09:54:57.617356 Epoch 17  	Train Loss = 2.26649 Val Loss = 2.82846
2025-02-20 09:55:41.727585 Epoch 18  	Train Loss = 2.26351 Val Loss = 2.72604
2025-02-20 09:56:25.792951 Epoch 19  	Train Loss = 2.24262 Val Loss = 2.68693
2025-02-20 09:57:10.018415 Epoch 20  	Train Loss = 2.21793 Val Loss = 2.79425
2025-02-20 09:57:54.299887 Epoch 21  	Train Loss = 2.22749 Val Loss = 2.71662
2025-02-20 09:58:38.465539 Epoch 22  	Train Loss = 2.16685 Val Loss = 2.69618
2025-02-20 09:59:22.698279 Epoch 23  	Train Loss = 2.20724 Val Loss = 2.72027
2025-02-20 10:00:06.822707 Epoch 24  	Train Loss = 2.16901 Val Loss = 2.71778
2025-02-20 10:00:50.956998 Epoch 25  	Train Loss = 2.15415 Val Loss = 2.67163
2025-02-20 10:01:35.069728 Epoch 26  	Train Loss = 2.15109 Val Loss = 2.93558
2025-02-20 10:02:19.337376 Epoch 27  	Train Loss = 2.13922 Val Loss = 2.68404
2025-02-20 10:03:03.404590 Epoch 28  	Train Loss = 2.12240 Val Loss = 2.74352
2025-02-20 10:03:47.539623 Epoch 29  	Train Loss = 2.12820 Val Loss = 2.73723
Early stopping at epoch: 29
Best at epoch 9:
Train Loss = 2.40534
Train MAE = 2.14452, RMSE = 4.39195, MAPE = 5.16824
Val Loss = 2.64918
Val MAE = 2.66675, RMSE = 5.54296, MAPE = 7.20105
Model checkpoint saved to: ../saved_models/STDN/STDN-PEMSD7M-2025-02-20-09-42-24.pt
--------- Test ---------
All Steps (1-12) MAE = 2.64561, RMSE = 5.45973, MAPE = 6.80383
Step 1 MAE = 1.41466, RMSE = 2.43641, MAPE = 3.32149
Step 2 MAE = 1.85310, RMSE = 3.40621, MAPE = 4.41999
Step 3 MAE = 2.17476, RMSE = 4.17470, MAPE = 5.31865
Step 4 MAE = 2.41194, RMSE = 4.76273, MAPE = 6.01157
Step 5 MAE = 2.60266, RMSE = 5.23170, MAPE = 6.60523
Step 6 MAE = 2.75581, RMSE = 5.60703, MAPE = 7.09889
Step 7 MAE = 2.87913, RMSE = 5.89970, MAPE = 7.49573
Step 8 MAE = 2.98663, RMSE = 6.14127, MAPE = 7.83478
Step 9 MAE = 3.07032, RMSE = 6.31295, MAPE = 8.08787
Step 10 MAE = 3.13843, RMSE = 6.44809, MAPE = 8.30394
Step 11 MAE = 3.20291, RMSE = 6.56231, MAPE = 8.49544
Step 12 MAE = 3.25700, RMSE = 6.65035, MAPE = 8.65235
Inference time: 4.37 s
